{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import language_tool_python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "\n",
    "lt_servers = []\n",
    "thread_list = []\n",
    "    \n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12978, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"training_set_rel3.xls\")\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving Essay Set #1\n",
      "Dataframe shape: (1800, 28)\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"essay_set\"]==2]\n",
    "print(f\"Retrieving Essay Set #{1}\")\n",
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "clean_df = df[['essay', 'domain1_score', 'domain2_score']].copy()\n",
    "clean_df['actual_score'] = clean_df['domain1_score'] + clean_df['domain2_score']\n",
    "clean_df.drop(['domain1_score', 'domain2_score'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>Certain materials being removed from libraries...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>Do you think that libraries should remove cert...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>In @DATE1's world, there are many things found...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>In life you have the 'offensive things'. The l...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay  actual_score\n",
       "1783  Certain materials being removed from libraries...           8.0\n",
       "1784  Write a persuasive essay to a newspaper reflec...           2.0\n",
       "1785  Do you think that libraries should remove cert...           5.0\n",
       "1786  In @DATE1's world, there are many things found...           8.0\n",
       "1787  In life you have the 'offensive things'. The l...           8.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essay structure\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def unique_word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    unique_words = set(words)\n",
    "\n",
    "    return len(unique_words)\n",
    "\n",
    "def sentence_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)\n",
    "\n",
    "def avg_word_len(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom thread\n",
    "class LanguageCheck(Thread):\n",
    "    # constructor\n",
    "    def __init__(self, df, idx):\n",
    "        # execute the base constructor\n",
    "        Thread.__init__(self)\n",
    "        # set a default value\n",
    "        self.value = None\n",
    "        self.df = df\n",
    "        self.index = idx\n",
    " \n",
    "    # function executed in a new thread\n",
    "    def run(self):\n",
    "        self.df['grammar_errors'] = self.df['essay'].apply(self.grammar_errors)\n",
    "        self.value = self.df\n",
    "    \n",
    "    def grammar_errors(self, essay):\n",
    "        errors = lt_servers[self.index].check(essay)\n",
    "        return len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix errors\n",
    "def autocorrect_essay(essay):\n",
    "    language_tool = language_tool_python.LanguageTool('en-US')\n",
    "    corrected_essay = language_tool.correct(essay)\n",
    "    return corrected_essay\n",
    "\n",
    "# pool autocorrect\n",
    "def pool_autocorrect(essay):\n",
    "    language_tool = language_tool_python.LanguageTool('en-US')\n",
    "    corrected_essay = language_tool.correct(essay)\n",
    "    return corrected_essay\n",
    "\n",
    "df_split = np.array_split(clean_df, len(lt_servers))\n",
    "\n",
    "# put threads into list\n",
    "for idx, i in enumerate(df_split):\n",
    "    thread_langcheck = LanguageCheck(df=i, idx=idx)\n",
    "    thread_list.append(thread_langcheck)\n",
    "\n",
    "# start thread list\n",
    "for thread in thread_list:\n",
    "    thread.start()\n",
    "\n",
    "# join all threads\n",
    "for thread in thread_list:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "      <th>grammar_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>Dear @CAPS1, @CAPS2 several reasons on way I t...</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>Do a adults and kids spend to much time on the...</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>My opinion is that people should have computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>Dear readers, I think that its good and bad to...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>Dear - Local Newspaper I agree thats computers...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1783 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay  actual_score  \\\n",
       "0     Dear local newspaper, I think effects computer...             8   \n",
       "1     Dear @CAPS1 @CAPS2, I believe that using compu...             9   \n",
       "2     Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...             7   \n",
       "3     Dear Local Newspaper, @CAPS1 I have found that...            10   \n",
       "4     Dear @LOCATION1, I know having computers has a...             8   \n",
       "...                                                 ...           ...   \n",
       "1778  Dear @CAPS1, @CAPS2 several reasons on way I t...             8   \n",
       "1779  Do a adults and kids spend to much time on the...             7   \n",
       "1780  My opinion is that people should have computer...             8   \n",
       "1781  Dear readers, I think that its good and bad to...             2   \n",
       "1782  Dear - Local Newspaper I agree thats computers...             7   \n",
       "\n",
       "      grammar_errors  \n",
       "0                 16  \n",
       "1                 25  \n",
       "2                 17  \n",
       "3                 29  \n",
       "4                 17  \n",
       "...              ...  \n",
       "1778              33  \n",
       "1779              16  \n",
       "1780              15  \n",
       "1781               0  \n",
       "1782              20  \n",
       "\n",
       "[1783 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = pd.concat([thread.value for thread in thread_list], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, essay_set):\n",
    "    df = df[df[\"essay_set\"]==essay_set]\n",
    "    print(f\"Retrieving Essay Set #{essay_set}\")\n",
    "    print(f\"Dataframe shape: {df.shape}\")\n",
    "    clean_df = df[['essay', 'domain1_score']].copy()\n",
    "    clean_df = clean_df.rename(columns={'domain1_score': 'actual_score'})\n",
    "\n",
    "    # get essay structure\n",
    "    print(\"Getting Word Count\")\n",
    "    clean_df['word_count'] = clean_df['essay'].apply(word_count)\n",
    "    print(\"Getting Unique Word Count\")\n",
    "    clean_df['unique_word_count'] = clean_df['essay'].apply(unique_word_count)\n",
    "    print(\"Getting Sentence Count\")\n",
    "    clean_df['sentence_count'] = clean_df['essay'].apply(sentence_count)\n",
    "    print(\"Getting Average Word Length\")\n",
    "    clean_df['avg_word_len'] = clean_df['essay'].apply(avg_word_len)\n",
    "\n",
    "    # get grammatical errors\n",
    "    print(\"Getting Grammatical Errors\")\n",
    "    clean_df['grammar_errors'] = clean_df['essay'].apply(grammar_errors)\n",
    "\n",
    "    # autocorrect errors\n",
    "    print(\"Autocorrecting Essay\")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(autocorrect_essay)\n",
    "\n",
    "    # preprocess essay for tokenization\n",
    "    print(\"Preprocess for tokenization\")\n",
    "    clean_df.reset_index(drop=True, inplace=True)\n",
    "    clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenization\n",
    "    print(\"Tokenization Start\")\n",
    "    tokenized_doc = clean_df['essay'].apply(lambda x: x.split())\n",
    "\n",
    "    # remove stop-words\n",
    "    print(\"Removing Stop Words\")\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    # stemming\n",
    "    print(\"Word Stemming\")\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "    # de-tokenization\n",
    "    print(\"Detokenize\")\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(clean_df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    clean_df['essay'] = detokenized_doc\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_no_lsa_similarity():\n",
    "    # Essay Set 1, max_features = 10000, min_df = 5\n",
    "\n",
    "    essay_set = 1\n",
    "    max_features = 10000\n",
    "    min_df = 5\n",
    "\n",
    "    print(\"Preprocess Start\")\n",
    "    clean_df = preprocess_dataframe(df, essay_set)\n",
    "\n",
    "    print(\"Creating TF-IDF Vectorizer\")\n",
    "    # Create a vectorizer for the training data\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Vectorize document using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                            stop_words='english',\n",
    "                                            ngram_range = (1,3),\n",
    "                                            tokenizer = tokenizer.tokenize,\n",
    "                                            max_features=max_features,\n",
    "                                            max_df=0.8,\n",
    "                                            min_df=min_df)\n",
    "    print(\"Building Matrix\")\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(clean_df[\"essay\"])\n",
    "    print(f\"Train TFIDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "    print(\"Convert TF-IDF matrix to SVD\")\n",
    "    # TFIDF to SVD\n",
    "    svd_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "        \n",
    "    svd = svd_model.fit_transform(tfidf_matrix)\n",
    "    #normalized_svd = Normalizer(copy=False).fit_transform(svd)\n",
    "\n",
    "    print(\"Training Start\")\n",
    "\n",
    "    print(\"Getting Features\")\n",
    "    x_df_features = clean_df[['word_count', 'unique_word_count', 'sentence_count', 'avg_word_len', 'grammar_errors']]\n",
    "    x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "    y_features = clean_df['actual_score'].to_numpy()\n",
    "\n",
    "    print(\"Splitting Dataset\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)\n",
    "\n",
    "    print(\"Building Linear Regression Model\")\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building SVR Model\")\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Decision Tree Model\")\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Bayesian Regressor\")\n",
    "    bayes_model = BayesianRidge()\n",
    "    bayes_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building AdaBoost Regressor\")\n",
    "    ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "    ada_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Random Forest Regressor\")\n",
    "    ran_model = RandomForestRegressor()\n",
    "    ran_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Gradient Boosting Regressor\")\n",
    "    grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "    grad_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Logistic Regression Model\")\n",
    "    log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "    log_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Getting Predictions\")\n",
    "    predictions = [ lr_model.predict(x_test),\n",
    "                    svr_model.predict(x_test),\n",
    "                    tree_model.predict(x_test),\n",
    "                    bayes_model.predict(x_test),\n",
    "                    ada_model.predict(x_test),\n",
    "                    ran_model.predict(x_test),\n",
    "                    grad_model.predict(x_test),\n",
    "                    log_model.predict(x_test)]\n",
    "    scores = []\n",
    "    \n",
    "    for idx, pred in enumerate(predictions):\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_score = r2_score(y_test, pred)\n",
    "\n",
    "        scores.append([idx, mae, mse, rmse, r_score])\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    best_score = max(scores, key=lambda sublist: sublist[0])\n",
    "    print(f\"Model {best_score[0]}\")\n",
    "    print(f\"Mean Absolute Error: {best_score[1]}\")\n",
    "    print(f\"Mean Squared Error: {best_score[2]}\")\n",
    "    print(f\"Root Mean Squared Error: {best_score[3]}\")\n",
    "    print(f\"R2 score: {best_score[4]}\\n\")\n",
    "\n",
    "    print(\"Cross Validation 10-Folds\")\n",
    "    scores = [cross_val_score(lr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(svr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(tree_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(bayes_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ada_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ran_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(grad_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(log_model, x_train, y_train, cv=10).mean()]\n",
    "    \n",
    "    print(f\"Model {scores.index(max(scores))}\")\n",
    "    print(f\"Overall Score: {max(scores)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess Start\n",
      "Retrieving Essay Set #1\n",
      "Dataframe shape: (1783, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "Getting Grammatical Errors\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n",
      "THREAD!\n"
     ]
    }
   ],
   "source": [
    "scorer_no_lsa_similarity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d888b7e27d9d4f8fccada6ae7e4260d620cc5227dee391d6d7538000bf76028"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
