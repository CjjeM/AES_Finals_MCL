{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import language_tool_python\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persuasive/Narrative/Expository Essays Summary\n",
    "## Essay Set 1, 2, 7, 8\n",
    "\n",
    "Features Used\n",
    "- Topic Relevance\n",
    "    - LSA (TF-IDF) Matrix\n",
    "    - Cosine similarity between an essay to the top scoring essays by their TruncatedSVD Matrix\n",
    "- Word Usage and Sentence complexity\n",
    "    - Number of words, Sentences, Unique words, Average word length\n",
    "    - Parts-Of-Speech Tagging\n",
    "- Grammar and Mechanics\n",
    "    - Language Tool (number of mistakes)\n",
    "- Readability (Text Complexity)\n",
    "    - Flesch Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread for LanguageTool\n",
    "class LanguageCorrect(Thread):\n",
    "    def __init__(self, df, idx, lt_servers):\n",
    "        Thread.__init__(self)\n",
    "        self.value = None\n",
    "        self.df = df\n",
    "        self.index = idx\n",
    "        self.lt_servers = lt_servers\n",
    " \n",
    "    def run(self):\n",
    "        self.df['essay'] = self.df['essay'].apply(self.autocorrect_essay)\n",
    "        self.value = self.df\n",
    "        return\n",
    "    \n",
    "    def autocorrect_essay(self, essay):\n",
    "        corrected_essay = self.lt_servers[self.index].correct(essay)\n",
    "        return corrected_essay\n",
    "\n",
    "class LanguageCheck(Thread):\n",
    "    def __init__(self, df, idx, lt_servers):\n",
    "        Thread.__init__(self)\n",
    "        self.value = None\n",
    "        self.df = df\n",
    "        self.index = idx\n",
    "        self.lt_servers = lt_servers\n",
    "\n",
    "    def run(self):\n",
    "        self.df['grammar_errors'] = self.df['essay'].apply(self.grammar_errors)\n",
    "        self.value = self.df\n",
    "        return\n",
    "    \n",
    "    def grammar_errors(self, essay):\n",
    "        errors = self.lt_servers[self.index].check(essay)\n",
    "        return len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essay structure\n",
    "\n",
    "def word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def unique_word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    unique_words = set(words)\n",
    "\n",
    "    return len(unique_words)\n",
    "\n",
    "def sentence_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)\n",
    "\n",
    "def avg_word_len(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "\n",
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def count_pos(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "            \n",
    "    return noun_count, adj_count, verb_count, adv_count\n",
    "\n",
    "\n",
    "def readability_score(essay):\n",
    "    score = flesch_reading_ease(essay)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(essay_set):\n",
    "    lt_servers = []\n",
    "    for _ in range(3):\n",
    "        lt_servers.append(language_tool_python.LanguageTool('en-US'))\n",
    "\n",
    "    thread_list = []\n",
    "\n",
    "    df = pd.read_excel(\"training_set_rel3.xls\")\n",
    "    df = df[df[\"essay_set\"]==essay_set]\n",
    "\n",
    "    print(f\"Retrieving Essay Set: #{essay_set}\")\n",
    "    print(f\"DataFrame Shape: {df.shape}\")\n",
    "\n",
    "    clean_df = df[['essay', 'domain1_score']].copy()\n",
    "    clean_df = clean_df.rename(columns={'domain1_score': 'actual_score'})\n",
    "\n",
    "    if essay_set == 2:\n",
    "        clean_df = df[['essay', 'domain1_score', 'domain2_score']].copy()\n",
    "        clean_df['actual_score'] = clean_df['domain1_score'] + clean_df['domain2_score']\n",
    "        clean_df.drop(['domain1_score', 'domain2_score'], axis=1, inplace=True)\n",
    "\n",
    "    # get essay structure\n",
    "    print(\"\\n- PREPROCESSING ESSAY SETS -\\n\")\n",
    "    print(\"Getting Word Count..\")\n",
    "    clean_df['word_count'] = clean_df['essay'].apply(word_count)\n",
    "    print(\"Getting Unique Word Count..\")\n",
    "    clean_df['unique_word_count'] = clean_df['essay'].apply(unique_word_count)\n",
    "    print(\"Getting Sentence Count..\")\n",
    "    clean_df['sentence_count'] = clean_df['essay'].apply(sentence_count)\n",
    "    print(\"Getting Average Word Length..\")\n",
    "    clean_df['avg_word_len'] = clean_df['essay'].apply(avg_word_len)\n",
    "    print(\"POS Tagging..\")\n",
    "    clean_df['noun_count'], clean_df['adj_count'], clean_df['verb_count'], clean_df['adv_count'] = zip(*clean_df['essay'].map(count_pos))\n",
    "    print(\"Getting Readability..\")\n",
    "    clean_df['readability_score'] = clean_df['essay'].apply(readability_score)\n",
    "\n",
    "    # get grammatical errors\n",
    "    print(\"Getting Grammatical Errors..\")\n",
    "    df_split = np.array_split(clean_df, len(lt_servers))\n",
    "    # put threads into list\n",
    "    for idx, i in enumerate(df_split):\n",
    "        thread_langcheck = LanguageCheck(df=i, idx=idx, lt_servers=lt_servers)\n",
    "        thread_list.append(thread_langcheck)\n",
    "\n",
    "    # start thread list\n",
    "    for thread in thread_list:\n",
    "        thread.start()\n",
    "\n",
    "    # join all threads\n",
    "    for thread in thread_list:\n",
    "        thread.join()\n",
    "    \n",
    "    clean_df = pd.concat([thread.value for thread in thread_list], axis=0)\n",
    "    \n",
    "    thread_list.clear()\n",
    "\n",
    "    # autocorrect errors\n",
    "    print(\"Autocorrecting Essay..\")\n",
    "    df_split = np.array_split(clean_df, len(lt_servers))\n",
    "    # put threads into list\n",
    "    for idx, i in enumerate(df_split):\n",
    "        thread_langcheck = LanguageCorrect(df=i, idx=idx, lt_servers=lt_servers)\n",
    "        thread_list.append(thread_langcheck)\n",
    "\n",
    "    # start thread list\n",
    "    for thread in thread_list:\n",
    "        thread.start()\n",
    "\n",
    "    # join all threads\n",
    "    for thread in thread_list:\n",
    "        thread.join()\n",
    "    \n",
    "    clean_df = pd.concat([thread.value for thread in thread_list], axis=0)\n",
    "\n",
    "    for tool in lt_servers:\n",
    "        tool.close()\n",
    "    \n",
    "    thread_list.clear()\n",
    "    lt_servers.clear()\n",
    "\n",
    "    # preprocess essay for tokenization\n",
    "    clean_df.reset_index(drop=True, inplace=True)\n",
    "    clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenization\n",
    "    print(\"Tokenizing..\")\n",
    "    tokenized_doc = clean_df['essay'].apply(lambda x: x.split())\n",
    "\n",
    "    # remove stop-words\n",
    "    print(\"Removing Stop Words..\")\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    # stemming\n",
    "    print(\"Stemming Words..\")\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "    # de-tokenization\n",
    "    print(\"Detokenizing..\")\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(clean_df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    clean_df['essay'] = detokenized_doc\n",
    "    print(\"\\n- FINISHED -\\n\")\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization_process(df, sample_essays, max_features):\n",
    "    df_lsa = df.copy()\n",
    "    largest_possible_score = df_lsa.nlargest(1, 'actual_score')['actual_score'].values[0]\n",
    "\n",
    "    top_score = largest_possible_score - (largest_possible_score * 0.10)\n",
    "\n",
    "    chosen_essay = df_lsa[df_lsa['actual_score'] >= top_score]\n",
    "    chosen_essay = chosen_essay.groupby('actual_score').sample(sample_essays, random_state=26)\n",
    "\n",
    "    df_lsa = df_lsa.drop(index = chosen_essay.index)\n",
    "\n",
    "    # Create a vectorizer for lsa similarity\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Vectorize document using TF-IDF\n",
    "    tfidf_lsa_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                            stop_words='english',\n",
    "                                            ngram_range = (1,3),\n",
    "                                            tokenizer = tokenizer.tokenize)\n",
    "\n",
    "    tfidf_lsa_matrix = tfidf_lsa_vectorizer.fit_transform(chosen_essay[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_lsa_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "        \n",
    "    svd_lsa = svd_lsa_model.fit_transform(tfidf_lsa_matrix)\n",
    "    normalized_svd = Normalizer(copy=False).fit_transform(svd_lsa)\n",
    "\n",
    "    def lsa_score(essay):\n",
    "        essay_matrix = tfidf_lsa_vectorizer.transform([essay])\n",
    "        essay_svd = svd_lsa_model.transform(essay_matrix)\n",
    "        normalized_essay_svd = Normalizer(copy=False).fit_transform(essay_svd)\n",
    "\n",
    "        # Compare current essay to the top 10% scored essay\n",
    "        similarities = cosine_similarity(normalized_svd, normalized_essay_svd).max()\n",
    "\n",
    "        return similarities.max()\n",
    "    \n",
    "    df_lsa['lsa_score'] = df_lsa['essay'].apply(lsa_score)\n",
    "\n",
    "    # Create a vectorizer for the training data\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Vectorize document using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                    stop_words='english',\n",
    "                                    ngram_range = (1,3),\n",
    "                                    tokenizer = tokenizer.tokenize,\n",
    "                                    max_features=max_features)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_lsa[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "        \n",
    "    svd = svd_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    return df_lsa, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process(df_lsa, svd):\n",
    "    print(\"Getting Features\")\n",
    "    x_df_features = df_lsa[['word_count', \n",
    "                            'unique_word_count',\n",
    "                            'sentence_count',\n",
    "                            'avg_word_len',\n",
    "                            'grammar_errors',\n",
    "                            'lsa_score', \n",
    "                            'readability_score',\n",
    "                            'noun_count',\n",
    "                            'adj_count',\n",
    "                            'verb_count',\n",
    "                            'adv_count']]\n",
    "\n",
    "    x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "    y_features = df_lsa['actual_score'].to_numpy()\n",
    "\n",
    "    print(\"Splitting Dataset\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)\n",
    "\n",
    "    print(\"Building Linear Regression Model\")\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building SVR Model\")\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Decision Tree Model\")\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Bayesian Regressor\")\n",
    "    bayes_model = BayesianRidge()\n",
    "    bayes_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building AdaBoost Regressor\")\n",
    "    ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "    ada_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Random Forest Regressor\")\n",
    "    ran_model = RandomForestRegressor()\n",
    "    ran_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Gradient Boosting Regressor\")\n",
    "    grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "    grad_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Logistic Regression Model\")\n",
    "    log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "    log_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Getting Predictions\")\n",
    "    predictions = [ lr_model.predict(x_test),\n",
    "                    svr_model.predict(x_test),\n",
    "                    tree_model.predict(x_test),\n",
    "                    bayes_model.predict(x_test),\n",
    "                    ada_model.predict(x_test),\n",
    "                    ran_model.predict(x_test),\n",
    "                    grad_model.predict(x_test),\n",
    "                    log_model.predict(x_test)]\n",
    "    scores = []\n",
    "    \n",
    "    for idx, pred in enumerate(predictions):\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_score = r2_score(y_test, pred)\n",
    "\n",
    "        scores.append([idx, mae, mse, rmse, r_score])\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    best_score = max(scores, key=lambda sublist: sublist[-1])\n",
    "    print(f\"Model {best_score[0]}\")\n",
    "    print(f\"Mean Absolute Error: {best_score[1]}\")\n",
    "    print(f\"Mean Squared Error: {best_score[2]}\")\n",
    "    print(f\"Root Mean Squared Error: {best_score[3]}\")\n",
    "    print(f\"R2 score: {best_score[4]}\\n\")\n",
    "\n",
    "    print(\"Cross Validation 10-Folds\")\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    scores = [cross_val_score(lr_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(svr_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(tree_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(bayes_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(ada_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(ran_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(grad_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(log_model, x_features, y_features, cv=kf).mean()]\n",
    "    \n",
    "    print(f\"Model {scores.index(max(scores))}\")\n",
    "    print(f\"Overall Score: {max(scores)}\\n\")\n",
    "\n",
    "    return best_score, (scores.index(max(scores)), max(scores))# average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess Start\n",
      "Retrieving Essay Set #1\n",
      "Dataframe shape: (1783, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 5\n",
      "Mean Absolute Error: 0.5931728045325778\n",
      "Mean Squared Error: 0.5577917847025495\n",
      "Root Mean Squared Error: 0.746854594082777\n",
      "R2 score: 0.7728462200885008\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 1\n",
      "Overall Score: 0.7318932133096692\n",
      "\n",
      "\n",
      "\n",
      "Preprocess Start\n",
      "Retrieving Essay Set #2\n",
      "Dataframe shape: (1800, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 6\n",
      "Mean Absolute Error: 0.6484774183609476\n",
      "Mean Squared Error: 0.6747706545099783\n",
      "Root Mean Squared Error: 0.8214442491794427\n",
      "R2 score: 0.5977613666762005\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 6\n",
      "Overall Score: 0.6286681761170474\n",
      "\n",
      "\n",
      "\n",
      "Preprocess Start\n",
      "Retrieving Essay Set #7\n",
      "Dataframe shape: (1569, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 6\n",
      "Mean Absolute Error: 2.0788689025580025\n",
      "Mean Squared Error: 6.874569049361013\n",
      "Root Mean Squared Error: 2.621939940075099\n",
      "R2 score: 0.6652576217071945\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 6\n",
      "Overall Score: 0.6322037224022616\n",
      "\n",
      "\n",
      "\n",
      "Preprocess Start\n",
      "Retrieving Essay Set #8\n",
      "Dataframe shape: (723, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 5\n",
      "Mean Absolute Error: 2.9186896551724137\n",
      "Mean Squared Error: 14.778853103448276\n",
      "Root Mean Squared Error: 3.8443273928540838\n",
      "R2 score: 0.5665732280751676\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 5\n",
      "Overall Score: 0.5403300813474997\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not source summary\n",
    "# (essay_set, sample_essay, max_features)\n",
    "summaries = [(1, 10, 10000),\n",
    "            (2, 5, 10000), \n",
    "            (7, 10, 10000), \n",
    "            (8, 1, 1000)]\n",
    "\n",
    "summary_scores = []\n",
    "\n",
    "for summary in summaries:\n",
    "    print(\"Preprocess Start\")\n",
    "    clean_df = preprocess_dataframe(summary[0])\n",
    "    \n",
    "    print(\"Vectorization Start\")\n",
    "    df_lsa, svd = vectorization_process(clean_df, summary[1], summary[2])\n",
    "    \n",
    "    print(\"Training Start\")\n",
    "    best_score, average_score = training_process(df_lsa, svd)\n",
    "\n",
    "    summary_scores.append([best_score, average_score])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Dependent Essays Summary\n",
    "## Essay Sets 3, 4, 5 and 6\n",
    "\n",
    "Features Used\n",
    "- Topic Relevance\n",
    "    - LSA (TF-IDF) Matrix\n",
    "    - Cosine similarity between an essay to the top scoring essays by their TruncatedSVD Matrix\n",
    "- Word Usage and Sentence complexity\n",
    "    - Number of words, Sentences, Unique words, Average word length\n",
    "    - Parts-Of-Speech Tagging\n",
    "- Grammar and Mechanics\n",
    "    - Language Tool (number of mistakes)\n",
    "- Readability (Text Complexity)\n",
    "    - Flesch Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_source_essay(essay_set):\n",
    "    source = pd.read_csv('source_essays.txt', sep=\"|\", header=None)\n",
    "    stacked_source = source.stack().reset_index()\n",
    "    source_essay = stacked_source.drop(['level_0', 'level_1'], axis=1).rename(columns={0: 'essay'})\n",
    "    source_essay.insert(0, \"essay_set\", [6, 5, 4, 3], True)\n",
    "    source_essay = source_essay.sort_values(by=['essay_set'], ascending=True)\n",
    "    source_essay = source_essay.loc[source_essay['essay_set'] == essay_set]\n",
    "\n",
    "    source_essay.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"\\n- PREPROCESSING SOURCE ESSAY -\\n\")\n",
    "    print(\"Tokenizing..\")\n",
    "    source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    source_essay['essay'] = source_essay['essay'].apply(lambda x: x.lower())\n",
    "\n",
    "    tokenized_doc = source_essay['essay'].apply(lambda x: x.split())\n",
    "\n",
    "    print(\"Removing Stop Words..\")\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    print(\"Stemming Words..\")\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "    print(\"Detokenizing..\")\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(source_essay)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    source_essay['essay'] = detokenized_doc\n",
    "    print(\"\\n- FINISHED -\\n\")\n",
    "    \n",
    "    return source_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_vectorization_process(df, sdf, sample_essays, max_features):\n",
    "    df_lsa = df.copy()\n",
    "    largest_possible_score = df_lsa.nlargest(1, 'actual_score')['actual_score'].values[0]\n",
    "    top_score = largest_possible_score - (largest_possible_score * 0.10)\n",
    "\n",
    "    df_source = sdf.copy()\n",
    "    frames = [df_lsa, df_source]\n",
    "    combined_df = pd.concat(frames)\n",
    "\n",
    "    combined_essay = combined_df[combined_df['actual_score'] >= top_score]\n",
    "    combined_essay = combined_essay.groupby('actual_score').sample(sample_essays, random_state=26)\n",
    "    combined_df = combined_df.drop(index = combined_essay.index)\n",
    "\n",
    "    # Create a vectorizer for LSA Similarity\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Vectorize using TF-IDF\n",
    "    tfidf_lsa_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                            stop_words='english',\n",
    "                                            ngram_range = (1,3),\n",
    "                                            tokenizer = tokenizer.tokenize)\n",
    "\n",
    "    tfidf_lsa_matrix = tfidf_lsa_vectorizer.fit_transform(combined_essay[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_lsa_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "        \n",
    "    svd_lsa = svd_lsa_model.fit_transform(tfidf_lsa_matrix)\n",
    "    normalized_svd = Normalizer(copy=False).fit_transform(svd_lsa)\n",
    "\n",
    "    def lsa_score(essay):\n",
    "        essay_matrix = tfidf_lsa_vectorizer.transform([essay])\n",
    "        essay_svd = svd_lsa_model.transform(essay_matrix)\n",
    "        normalized_essay_svd = Normalizer(copy=False).fit_transform(essay_svd)\n",
    "\n",
    "        # Compare current essay to the top 10% scored essay\n",
    "        similarities = cosine_similarity(normalized_svd, normalized_essay_svd).max()\n",
    "\n",
    "        return similarities.max()\n",
    "    \n",
    "    combined_df['lsa_score'] = combined_df['essay'].apply(lsa_score)\n",
    "    combined_df = combined_df.fillna(0)\n",
    "\n",
    "    # VECTORIZE: Training Data\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                    stop_words='english',\n",
    "                                    ngram_range = (1,3),\n",
    "                                    tokenizer = tokenizer.tokenize,\n",
    "                                    max_features=max_features)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(combined_df[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "   \n",
    "    svd = svd_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    return combined_df, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_training_process(df_lsa, svd):\n",
    "    print(\"Getting Features..\")\n",
    "    x_df_features = df_lsa[['word_count', \n",
    "                            'unique_word_count',\n",
    "                            'sentence_count',\n",
    "                            'avg_word_len',\n",
    "                            'grammar_errors',\n",
    "                            'lsa_score', \n",
    "                            'readability_score',\n",
    "                            'noun_count',\n",
    "                            'adj_count',\n",
    "                            'verb_count',\n",
    "                            'adv_count']]\n",
    "\n",
    "    x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "    y_features = df_lsa['actual_score'].to_numpy()\n",
    "\n",
    "    print(\"Splitting Dataset..\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)\n",
    "\n",
    "    print(\"Building Linear Regression Model..\")\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building SVR Model..\")\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Decision Tree Model..\")\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Bayesian Regressor..\")\n",
    "    bayes_model = BayesianRidge()\n",
    "    bayes_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building AdaBoost Regressor..\")\n",
    "    ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "    ada_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Random Forest Regressor..\")\n",
    "    ran_model = RandomForestRegressor()\n",
    "    ran_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Gradient Boosting Regressor..\")\n",
    "    grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "    grad_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Logistic Regression Model..\")\n",
    "    log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "    log_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Getting Predictions..\")\n",
    "    predictions = [ lr_model.predict(x_test),\n",
    "                    svr_model.predict(x_test),\n",
    "                    tree_model.predict(x_test),\n",
    "                    bayes_model.predict(x_test),\n",
    "                    ada_model.predict(x_test),\n",
    "                    ran_model.predict(x_test),\n",
    "                    grad_model.predict(x_test),\n",
    "                    log_model.predict(x_test)]\n",
    "    scores = []\n",
    "    \n",
    "    for idx, pred in enumerate(predictions):\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_score = r2_score(y_test, pred)\n",
    "\n",
    "        scores.append([idx, mae, mse, rmse, r_score])\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    best_score = max(scores, key=lambda sublist: sublist[-1])\n",
    "    print(f\"MODEL: {best_score[0]}\")\n",
    "    print(f\"Mean Absolute Error: {best_score[1]}\")\n",
    "    print(f\"Mean Squared Error: {best_score[2]}\")\n",
    "    print(f\"Root Mean Squared Error: {best_score[3]}\")\n",
    "    print(f\"R2 score: {best_score[4]}\\n\")\n",
    "\n",
    "    print(\"[Cross Validation 10-Folds]\")\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    scores = [cross_val_score(lr_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(svr_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(tree_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(bayes_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(ada_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(ran_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(grad_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(log_model, x_features, y_features, cv=kf).mean()]\n",
    "    \n",
    "    print(f\"MODEL: {scores.index(max(scores))}\")\n",
    "    print(f\"Overall Score: {max(scores)}\\n\")\n",
    "\n",
    "    return best_score, (scores.index(max(scores)), max(scores))# average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving Essay Set: #3\n",
      "DataFrame Shape: (1726, 28)\n",
      "\n",
      "- PREPROCESSING ESSAY SETS -\n",
      "\n",
      "Getting Word Count..\n",
      "Getting Unique Word Count..\n",
      "Getting Sentence Count..\n",
      "Getting Average Word Length..\n",
      "POS Tagging..\n",
      "Getting Readability..\n",
      "Getting Grammatical Errors..\n",
      "Autocorrecting Essay..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\2340480230.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "\n",
      "- PREPROCESSING SOURCE ESSAY -\n",
      "\n",
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\674088542.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>readability_score</th>\n",
       "      <th>grammar_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>featur set affect cyclist mani way featur set ...</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>4.098039</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>71.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>featur set affect cyclist neg desert dri hot t...</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>106</td>\n",
       "      <td>12</td>\n",
       "      <td>4.418994</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>70.63</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everyon travel unfamiliar place sometim get lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>73</td>\n",
       "      <td>8</td>\n",
       "      <td>4.164948</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>83.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>believ featur cyclist affect impati trustworth...</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>3.896552</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>67.72</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>set effect cyclist set differ stori would made...</td>\n",
       "      <td>2</td>\n",
       "      <td>134</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>4.126866</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>63.06</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>stori set affect cyclist mani way exampl condi...</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>4.136364</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>85.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>featur set affect cyclist like group hill alon...</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>78.59</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>set greatli affect cyclist tri get yosemit nat...</td>\n",
       "      <td>2</td>\n",
       "      <td>113</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>4.159292</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>69.62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>featur set affect cyclist author say californi...</td>\n",
       "      <td>2</td>\n",
       "      <td>152</td>\n",
       "      <td>90</td>\n",
       "      <td>7</td>\n",
       "      <td>4.302632</td>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>66.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>featur set rough road ahead exceed post speed ...</td>\n",
       "      <td>3</td>\n",
       "      <td>119</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>4.277311</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>64.44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay  actual_score  \\\n",
       "0     featur set affect cyclist mani way featur set ...             1   \n",
       "1     featur set affect cyclist neg desert dri hot t...             2   \n",
       "2     everyon travel unfamiliar place sometim get lo...             1   \n",
       "3     believ featur cyclist affect impati trustworth...             1   \n",
       "4     set effect cyclist set differ stori would made...             2   \n",
       "...                                                 ...           ...   \n",
       "1721  stori set affect cyclist mani way exampl condi...             2   \n",
       "1722  featur set affect cyclist like group hill alon...             1   \n",
       "1723  set greatli affect cyclist tri get yosemit nat...             2   \n",
       "1724  featur set affect cyclist author say californi...             2   \n",
       "1725  featur set rough road ahead exceed post speed ...             3   \n",
       "\n",
       "      word_count  unique_word_count  sentence_count  avg_word_len  noun_count  \\\n",
       "0             51                 32               3      4.098039          15   \n",
       "1            179                106              12      4.418994          49   \n",
       "2             97                 73               8      4.164948          22   \n",
       "3             87                 62               3      3.896552          19   \n",
       "4            134                 80               3      4.126866          31   \n",
       "...          ...                ...             ...           ...         ...   \n",
       "1721          66                 52               6      4.136364          19   \n",
       "1722          54                 48               3      4.333333          15   \n",
       "1723         113                 74               5      4.159292          26   \n",
       "1724         152                 90               7      4.302632          33   \n",
       "1725         119                 83               5      4.277311          34   \n",
       "\n",
       "      adj_count  verb_count  adv_count  readability_score  grammar_errors  \n",
       "0             1           9          0              71.14               1  \n",
       "1            13          29          7              70.63              19  \n",
       "2             8          19         11              83.05               1  \n",
       "3            11          13          3              67.72              14  \n",
       "4            15          21          8              63.06              11  \n",
       "...         ...         ...        ...                ...             ...  \n",
       "1721          5          14          1              85.69               1  \n",
       "1722          5          10          7              78.59               2  \n",
       "1723          7          21          7              69.62               3  \n",
       "1724         11          30         15              66.67               2  \n",
       "1725          8          18          6              64.44               2  \n",
       "\n",
       "[1726 rows x 12 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = preprocess_dataframe(3)\n",
    "source_df = preprocess_source_essay(3)\n",
    "combined_df, svd = sd_vectorization_process(clean_df, source_df, 10, 10000)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: Preprocessing..\n",
      "\n",
      "Retrieving Essay Set: #3\n",
      "DataFrame Shape: (1726, 28)\n",
      "\n",
      "- PREPROCESSING ESSAY SETS -\n",
      "\n",
      "Getting Word Count..\n",
      "Getting Unique Word Count..\n",
      "Getting Sentence Count..\n",
      "Getting Average Word Length..\n",
      "POS Tagging..\n",
      "Getting Readability..\n",
      "Getting Grammatical Errors..\n",
      "Autocorrecting Essay..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\2340480230.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "\n",
      "- PREPROCESSING SOURCE ESSAY -\n",
      "\n",
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "PROCESS: Vectorization..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\674088542.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Training..\n",
      "\n",
      "Getting Features\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['lsa_score'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mark Anthony Mamauag\\AES_Finals_MCL\\overall_summary.ipynb Cell 14\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m df_lsa\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPROCESS: Training..\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m best_score, average_score \u001b[39m=\u001b[39m training_process(df_lsa, svd)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m summary_scores\u001b[39m.\u001b[39mappend([best_score, average_score])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Mark Anthony Mamauag\\AES_Finals_MCL\\overall_summary.ipynb Cell 14\u001b[0m in \u001b[0;36mtraining_process\u001b[1;34m(df_lsa, svd)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_process\u001b[39m(df_lsa, svd):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGetting Features\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x_df_features \u001b[39m=\u001b[39m df_lsa[[\u001b[39m'\u001b[39;49m\u001b[39mword_count\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39munique_word_count\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39msentence_count\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39mavg_word_len\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39mgrammar_errors\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39mlsa_score\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39mreadability_score\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39mnoun_count\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39madj_count\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39mverb_count\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                             \u001b[39m'\u001b[39;49m\u001b[39madv_count\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((x_df_features\u001b[39m.\u001b[39mto_numpy(), svd), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mark%20Anthony%20Mamauag/AES_Finals_MCL/overall_summary.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     y_features \u001b[39m=\u001b[39m df_lsa[\u001b[39m'\u001b[39m\u001b[39mactual_score\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n",
      "File \u001b[1;32mc:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3512\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3510\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3511\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3512\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3514\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['lsa_score'] not in index\""
     ]
    }
   ],
   "source": [
    "sd_summaries = [(3, 10, 10000),\n",
    "            (4, 5, 10000), \n",
    "            (5, 10, 10000), \n",
    "            (6, 1, 1000)]\n",
    "\n",
    "sd_summary_scores = []\n",
    "\n",
    "for summary in sd_summaries:\n",
    "    print(\"START: Preprocessing..\\n\")\n",
    "    clean_df = preprocess_dataframe(summary[0])\n",
    "    source_df = preprocess_source_essay(summary[0])\n",
    "    \n",
    "    print(\"PROCESS: Vectorization..\\n\")\n",
    "    df_lsa, svd = sd_vectorization_process(clean_df, source_df, summary[1], summary[2])\n",
    "    df_lsa\n",
    "    \n",
    "    print(\"PROCESS: Training..\\n\")\n",
    "    best_score, average_score = training_process(df_lsa, svd)\n",
    "\n",
    "    summary_scores.append([best_score, average_score])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d4804b05511593ff5d39042ce4458bba250db19b4b42c28c2a085011e4afa5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
