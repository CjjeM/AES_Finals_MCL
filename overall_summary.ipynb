{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import language_tool_python\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persuasive/Narrative/Expository Essays Summary\n",
    "## Essay Set 1, 2, 7, 8\n",
    "\n",
    "Features Used\n",
    "- Topic Relevance\n",
    "    - LSA (TF-IDF) Matrix\n",
    "    - Cosine similarity between an essay to the top scoring essays by their TruncatedSVD Matrix\n",
    "- Word Usage and Sentence complexity\n",
    "    - Number of words, Sentences, Unique words, Average word length\n",
    "    - Parts-Of-Speech Tagging\n",
    "- Grammar and Mechanics\n",
    "    - Language Tool (number of mistakes)\n",
    "- Readability (Text Complexity)\n",
    "    - Flesch Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread for LanguageTool\n",
    "class LanguageCorrect(Thread):\n",
    "    def __init__(self, df, idx, lt_servers):\n",
    "        Thread.__init__(self)\n",
    "        self.value = None\n",
    "        self.df = df\n",
    "        self.index = idx\n",
    "        self.lt_servers = lt_servers\n",
    " \n",
    "    def run(self):\n",
    "        self.df['essay'] = self.df['essay'].apply(self.autocorrect_essay)\n",
    "        self.value = self.df\n",
    "        return\n",
    "    \n",
    "    def autocorrect_essay(self, essay):\n",
    "        corrected_essay = self.lt_servers[self.index].correct(essay)\n",
    "        return corrected_essay\n",
    "\n",
    "class LanguageCheck(Thread):\n",
    "    def __init__(self, df, idx, lt_servers):\n",
    "        Thread.__init__(self)\n",
    "        self.value = None\n",
    "        self.df = df\n",
    "        self.index = idx\n",
    "        self.lt_servers = lt_servers\n",
    "\n",
    "    def run(self):\n",
    "        self.df['grammar_errors'] = self.df['essay'].apply(self.grammar_errors)\n",
    "        self.value = self.df\n",
    "        return\n",
    "    \n",
    "    def grammar_errors(self, essay):\n",
    "        errors = self.lt_servers[self.index].check(essay)\n",
    "        return len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essay structure\n",
    "\n",
    "def word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def unique_word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    unique_words = set(words)\n",
    "\n",
    "    return len(unique_words)\n",
    "\n",
    "def sentence_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)\n",
    "\n",
    "def avg_word_len(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "\n",
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def count_pos(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "            \n",
    "    return noun_count, adj_count, verb_count, adv_count\n",
    "\n",
    "\n",
    "def readability_score(essay):\n",
    "    score = flesch_reading_ease(essay)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(essay_set):\n",
    "    lt_servers = []\n",
    "\n",
    "    ## Changed the value here (RAM Issues) ##\n",
    "    for _ in range(3):\n",
    "        lt_servers.append(language_tool_python.LanguageTool('en-US'))\n",
    "\n",
    "    thread_list = []\n",
    "\n",
    "    df = pd.read_excel(\"training_set_rel3.xls\")\n",
    "    df = df[df[\"essay_set\"]==essay_set]\n",
    "\n",
    "    print(f\"Retrieving Essay Set: #{essay_set}\")\n",
    "    print(f\"DataFrame Shape: {df.shape}\")\n",
    "\n",
    "    clean_df = df[['essay', 'domain1_score']].copy()\n",
    "    clean_df = clean_df.rename(columns={'domain1_score': 'actual_score'})\n",
    "\n",
    "    if essay_set == 2:\n",
    "        clean_df = df[['essay', 'domain1_score', 'domain2_score']].copy()\n",
    "        clean_df['actual_score'] = clean_df['domain1_score'] + clean_df['domain2_score']\n",
    "        clean_df.drop(['domain1_score', 'domain2_score'], axis=1, inplace=True)\n",
    "\n",
    "    # get essay structure\n",
    "    print(\"\\n- PREPROCESSING ESSAY SETS -\\n\")\n",
    "    print(\"Getting Word Count..\")\n",
    "    clean_df['word_count'] = clean_df['essay'].apply(word_count)\n",
    "    print(\"Getting Unique Word Count..\")\n",
    "    clean_df['unique_word_count'] = clean_df['essay'].apply(unique_word_count)\n",
    "    print(\"Getting Sentence Count..\")\n",
    "    clean_df['sentence_count'] = clean_df['essay'].apply(sentence_count)\n",
    "    print(\"Getting Average Word Length..\")\n",
    "    clean_df['avg_word_len'] = clean_df['essay'].apply(avg_word_len)\n",
    "    print(\"POS Tagging..\")\n",
    "    clean_df['noun_count'], clean_df['adj_count'], clean_df['verb_count'], clean_df['adv_count'] = zip(*clean_df['essay'].map(count_pos))\n",
    "    print(\"Getting Readability..\")\n",
    "    clean_df['readability_score'] = clean_df['essay'].apply(readability_score)\n",
    "\n",
    "    # get grammatical errors\n",
    "    print(\"Getting Grammatical Errors..\")\n",
    "    df_split = np.array_split(clean_df, len(lt_servers))\n",
    "    # put threads into list\n",
    "    for idx, i in enumerate(df_split):\n",
    "        thread_langcheck = LanguageCheck(df=i, idx=idx, lt_servers=lt_servers)\n",
    "        thread_list.append(thread_langcheck)\n",
    "\n",
    "    # start thread list\n",
    "    for thread in thread_list:\n",
    "        thread.start()\n",
    "\n",
    "    # join all threads\n",
    "    for thread in thread_list:\n",
    "        thread.join()\n",
    "    \n",
    "    clean_df = pd.concat([thread.value for thread in thread_list], axis=0)\n",
    "    \n",
    "    thread_list.clear()\n",
    "\n",
    "    # autocorrect errors\n",
    "    print(\"Autocorrecting Essay..\")\n",
    "    df_split = np.array_split(clean_df, len(lt_servers))\n",
    "    # put threads into list\n",
    "    for idx, i in enumerate(df_split):\n",
    "        thread_langcheck = LanguageCorrect(df=i, idx=idx, lt_servers=lt_servers)\n",
    "        thread_list.append(thread_langcheck)\n",
    "\n",
    "    # start thread list\n",
    "    for thread in thread_list:\n",
    "        thread.start()\n",
    "\n",
    "    # join all threads\n",
    "    for thread in thread_list:\n",
    "        thread.join()\n",
    "    \n",
    "    clean_df = pd.concat([thread.value for thread in thread_list], axis=0)\n",
    "\n",
    "    for tool in lt_servers:\n",
    "        tool.close()\n",
    "    \n",
    "    thread_list.clear()\n",
    "    lt_servers.clear()\n",
    "\n",
    "    # preprocess essay for tokenization\n",
    "    clean_df.reset_index(drop=True, inplace=True)\n",
    "    clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenization\n",
    "    print(\"Tokenizing..\")\n",
    "    tokenized_doc = clean_df['essay'].apply(lambda x: x.split())\n",
    "\n",
    "    # remove stop-words\n",
    "    print(\"Removing Stop Words..\")\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    # stemming\n",
    "    print(\"Stemming Words..\")\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "    # de-tokenization\n",
    "    print(\"Detokenizing..\")\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(clean_df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    clean_df['essay'] = detokenized_doc\n",
    "    print(\"\\n- FINISHED -\\n\")\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization_process(df, sample_essays, max_features):\n",
    "    df_lsa = df.copy()\n",
    "    largest_possible_score = df_lsa.nlargest(1, 'actual_score')['actual_score'].values[0]\n",
    "\n",
    "    top_score = largest_possible_score - (largest_possible_score * 0.10)\n",
    "\n",
    "    chosen_essay = df_lsa[df_lsa['actual_score'] >= top_score]\n",
    "    chosen_essay = chosen_essay.groupby('actual_score').sample(sample_essays, random_state=26)\n",
    "\n",
    "    df_lsa = df_lsa.drop(index = chosen_essay.index)\n",
    "\n",
    "    # Create a vectorizer for lsa similarity\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Vectorize document using TF-IDF\n",
    "    tfidf_lsa_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                            stop_words='english',\n",
    "                                            ngram_range = (1,3),\n",
    "                                            tokenizer = tokenizer.tokenize)\n",
    "\n",
    "    tfidf_lsa_matrix = tfidf_lsa_vectorizer.fit_transform(chosen_essay[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_lsa_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "        \n",
    "    svd_lsa = svd_lsa_model.fit_transform(tfidf_lsa_matrix)\n",
    "    normalized_svd = Normalizer(copy=False).fit_transform(svd_lsa)\n",
    "\n",
    "    def lsa_score(essay):\n",
    "        essay_matrix = tfidf_lsa_vectorizer.transform([essay])\n",
    "        essay_svd = svd_lsa_model.transform(essay_matrix)\n",
    "        normalized_essay_svd = Normalizer(copy=False).fit_transform(essay_svd)\n",
    "\n",
    "        # Compare current essay to the top 10% scored essay\n",
    "        similarities = cosine_similarity(normalized_svd, normalized_essay_svd).max()\n",
    "\n",
    "        return similarities.max()\n",
    "    \n",
    "    df_lsa['lsa_score'] = df_lsa['essay'].apply(lsa_score)\n",
    "\n",
    "    # Create a vectorizer for the training data\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Vectorize document using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                    stop_words='english',\n",
    "                                    ngram_range = (1,3),\n",
    "                                    tokenizer = tokenizer.tokenize,\n",
    "                                    max_features=max_features)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_lsa[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "        \n",
    "    svd = svd_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    return df_lsa, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process(df_lsa, svd):\n",
    "    print(\"Getting Features\")\n",
    "    x_df_features = df_lsa[['word_count', \n",
    "                            'unique_word_count',\n",
    "                            'sentence_count',\n",
    "                            'avg_word_len',\n",
    "                            'grammar_errors',\n",
    "                            'lsa_score', \n",
    "                            'readability_score',\n",
    "                            'noun_count',\n",
    "                            'adj_count',\n",
    "                            'verb_count',\n",
    "                            'adv_count']]\n",
    "\n",
    "    x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "    y_features = df_lsa['actual_score'].to_numpy()\n",
    "\n",
    "    print(\"Splitting Dataset..\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)\n",
    "\n",
    "    print(\"Building Linear Regression Model..\")\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building SVR Model..\")\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Decision Tree Model..\")\n",
    "    tree_model = DecisionTreeRegressor()\n",
    "    tree_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Bayesian Regressor..\")\n",
    "    bayes_model = BayesianRidge()\n",
    "    bayes_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building AdaBoost Regressor..\")\n",
    "    ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "    ada_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Random Forest Regressor..\")\n",
    "    ran_model = RandomForestRegressor()\n",
    "    ran_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Gradient Boosting Regressor..\")\n",
    "    grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "    grad_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Building Logistic Regression Model..\")\n",
    "    log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "    log_model.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Getting Predictions..\")\n",
    "    predictions = [ lr_model.predict(x_test),\n",
    "                    svr_model.predict(x_test),\n",
    "                    tree_model.predict(x_test),\n",
    "                    bayes_model.predict(x_test),\n",
    "                    ada_model.predict(x_test),\n",
    "                    ran_model.predict(x_test),\n",
    "                    grad_model.predict(x_test),\n",
    "                    log_model.predict(x_test)]\n",
    "    scores = []\n",
    "    \n",
    "    for idx, pred in enumerate(predictions):\n",
    "        mae = mean_absolute_error(y_test, pred)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_score = r2_score(y_test, pred)\n",
    "\n",
    "        scores.append([idx, mae, mse, rmse, r_score])\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    best_score = max(scores, key=lambda sublist: sublist[-1])\n",
    "    print(f\"MODEL: {best_score[0]}\")\n",
    "    print(f\"Mean Absolute Error: {best_score[1]}\")\n",
    "    print(f\"Mean Squared Error: {best_score[2]}\")\n",
    "    print(f\"Root Mean Squared Error: {best_score[3]}\")\n",
    "    print(f\"R2 score: {best_score[4]}\\n\")\n",
    "\n",
    "    print(\"[Cross Validation 10-Folds]\")\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    scores = [cross_val_score(lr_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(svr_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(tree_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(bayes_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(ada_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(ran_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(grad_model, x_features, y_features, cv=kf).mean(),\n",
    "          cross_val_score(log_model, x_features, y_features, cv=kf).mean()]\n",
    "    \n",
    "    print(f\"MODEL: {scores.index(max(scores))}\")\n",
    "    print(f\"Overall Score: {max(scores)}\\n\")\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    return best_score, (scores.index(max(scores)), max(scores)) # average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess Start\n",
      "Retrieving Essay Set #1\n",
      "Dataframe shape: (1783, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 5\n",
      "Mean Absolute Error: 0.5931728045325778\n",
      "Mean Squared Error: 0.5577917847025495\n",
      "Root Mean Squared Error: 0.746854594082777\n",
      "R2 score: 0.7728462200885008\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 1\n",
      "Overall Score: 0.7318932133096692\n",
      "\n",
      "\n",
      "\n",
      "Preprocess Start\n",
      "Retrieving Essay Set #2\n",
      "Dataframe shape: (1800, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 6\n",
      "Mean Absolute Error: 0.6484774183609476\n",
      "Mean Squared Error: 0.6747706545099783\n",
      "Root Mean Squared Error: 0.8214442491794427\n",
      "R2 score: 0.5977613666762005\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 6\n",
      "Overall Score: 0.6286681761170474\n",
      "\n",
      "\n",
      "\n",
      "Preprocess Start\n",
      "Retrieving Essay Set #7\n",
      "Dataframe shape: (1569, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 6\n",
      "Mean Absolute Error: 2.0788689025580025\n",
      "Mean Squared Error: 6.874569049361013\n",
      "Root Mean Squared Error: 2.621939940075099\n",
      "R2 score: 0.6652576217071945\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 6\n",
      "Overall Score: 0.6322037224022616\n",
      "\n",
      "\n",
      "\n",
      "Preprocess Start\n",
      "Retrieving Essay Set #8\n",
      "Dataframe shape: (723, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "POS Tagging\n",
      "Getting Readability\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1056\\3942059866.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n",
      "Vectorization Start\n",
      "Training Start\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 5\n",
      "Mean Absolute Error: 2.9186896551724137\n",
      "Mean Squared Error: 14.778853103448276\n",
      "Root Mean Squared Error: 3.8443273928540838\n",
      "R2 score: 0.5665732280751676\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 5\n",
      "Overall Score: 0.5403300813474997\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not source summary\n",
    "# (essay_set, sample_essay, max_features)\n",
    "summaries = [(1, 10, 10000),\n",
    "            (2, 5, 10000), \n",
    "            (7, 10, 10000), \n",
    "            (8, 1, 1000)]\n",
    "\n",
    "summary_scores = []\n",
    "\n",
    "for summary in summaries:\n",
    "    print(\"Preprocess Start\")\n",
    "    clean_df = preprocess_dataframe(summary[0])\n",
    "    \n",
    "    print(\"Vectorization Start\")\n",
    "    df_lsa, svd = vectorization_process(clean_df, summary[1], summary[2])\n",
    "    \n",
    "    print(\"Training Start\")\n",
    "    best_score, average_score = training_process(df_lsa, svd)\n",
    "\n",
    "    summary_scores.append([best_score, average_score])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Dependent Essays Summary\n",
    "## Essay Sets 3, 4, 5 and 6\n",
    "\n",
    "Features Used\n",
    "- Topic Relevance\n",
    "    - LSA (TF-IDF) Matrix\n",
    "    - Cosine similarity between an essay to the top scoring essays by their TruncatedSVD Matrix\n",
    "- Word Usage and Sentence complexity\n",
    "    - Number of words, Sentences, Unique words, Average word length\n",
    "    - Parts-Of-Speech Tagging\n",
    "- Grammar and Mechanics\n",
    "    - Language Tool (number of mistakes)\n",
    "- Readability (Text Complexity)\n",
    "    - Flesch Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_source_essay(essay_set):\n",
    "    source = pd.read_csv('source_essays.txt', sep=\"|\", header=None)\n",
    "    stacked_source = source.stack().reset_index()\n",
    "    source_essay = stacked_source.drop(['level_0', 'level_1'], axis=1).rename(columns={0: 'essay'})\n",
    "    source_essay.insert(0, \"essay_set\", [6, 5, 4, 3], True)\n",
    "    source_essay = source_essay.sort_values(by=['essay_set'], ascending=True)\n",
    "    source_essay = source_essay.loc[source_essay['essay_set'] == essay_set]\n",
    "\n",
    "    source_essay.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"\\n- PREPROCESSING SOURCE ESSAY -\\n\")\n",
    "    print(\"Tokenizing..\")\n",
    "    source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    source_essay['essay'] = source_essay['essay'].apply(lambda x: x.lower())\n",
    "\n",
    "    tokenized_doc = source_essay['essay'].apply(lambda x: x.split())\n",
    "\n",
    "    print(\"Removing Stop Words..\")\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    print(\"Stemming Words..\")\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "    print(\"Detokenizing..\")\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(source_essay)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    source_essay['essay'] = detokenized_doc\n",
    "    print(\"\\n- FINISHED -\\n\")\n",
    "    print(\"======================================================\")\n",
    "    \n",
    "    return source_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_vectorization_process(df, sdf, sample_essays, max_features):\n",
    "    df_lsa = df.copy()\n",
    "    largest_possible_score = df_lsa.nlargest(1, 'actual_score')['actual_score'].values[0]\n",
    "    top_score = largest_possible_score - (largest_possible_score * 0.10)\n",
    "\n",
    "    df_source = sdf.copy()\n",
    "    frames = [df_lsa, df_source]\n",
    "    combined_df = pd.concat(frames)\n",
    "\n",
    "    combined_essay = combined_df[combined_df['actual_score'] >= top_score]\n",
    "    combined_essay = combined_essay.groupby('actual_score').sample(sample_essays, random_state=26)\n",
    "    combined_df = combined_df.drop(index = combined_essay.index)\n",
    "\n",
    "    # Create a vectorizer for LSA Similarity\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Vectorize using TF-IDF\n",
    "    tfidf_lsa_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                            stop_words='english',\n",
    "                                            ngram_range = (1,3),\n",
    "                                            tokenizer = tokenizer.tokenize)\n",
    "\n",
    "    tfidf_lsa_matrix = tfidf_lsa_vectorizer.fit_transform(combined_essay[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_lsa_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "        \n",
    "    svd_lsa = svd_lsa_model.fit_transform(tfidf_lsa_matrix)\n",
    "    normalized_svd = Normalizer(copy=False).fit_transform(svd_lsa)\n",
    "\n",
    "    def lsa_score(essay):\n",
    "        essay_matrix = tfidf_lsa_vectorizer.transform([essay])\n",
    "        essay_svd = svd_lsa_model.transform(essay_matrix)\n",
    "        normalized_essay_svd = Normalizer(copy=False).fit_transform(essay_svd)\n",
    "\n",
    "        # Compare current essay to the top 10% scored essay\n",
    "        similarities = cosine_similarity(normalized_svd, normalized_essay_svd).max()\n",
    "\n",
    "        return similarities.max()\n",
    "    \n",
    "    combined_df['lsa_score'] = combined_df['essay'].apply(lsa_score)\n",
    "    combined_df = combined_df.fillna(0)\n",
    "\n",
    "    # VECTORIZE: Training Data\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                    stop_words='english',\n",
    "                                    ngram_range = (1,3),\n",
    "                                    tokenizer = tokenizer.tokenize,\n",
    "                                    max_features=max_features)\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(combined_df[\"essay\"])\n",
    "\n",
    "    # TFIDF to SVD\n",
    "    svd_model = TruncatedSVD(n_components=100,\n",
    "                            n_iter=200,\n",
    "                            random_state=69)\n",
    "   \n",
    "    svd = svd_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    return combined_df, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: Preprocessing..\n",
      "\n",
      "Retrieving Essay Set: #3\n",
      "DataFrame Shape: (1726, 28)\n",
      "\n",
      "- PREPROCESSING ESSAY SETS -\n",
      "\n",
      "Getting Word Count..\n",
      "Getting Unique Word Count..\n",
      "Getting Sentence Count..\n",
      "Getting Average Word Length..\n",
      "POS Tagging..\n",
      "Getting Readability..\n",
      "Getting Grammatical Errors..\n",
      "Autocorrecting Essay..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\2340480230.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "\n",
      "- PREPROCESSING SOURCE ESSAY -\n",
      "\n",
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "PROCESS: Vectorization..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\674088542.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Training..\n",
      "\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 5\n",
      "Mean Absolute Error: 0.44045542635658913\n",
      "Mean Squared Error: 0.33914421834625325\n",
      "Root Mean Squared Error: 0.5823609004270919\n",
      "R2 score: 0.5041884485295733\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 7\n",
      "Overall Score: 0.6744526043791649\n",
      "\n",
      "\n",
      "\n",
      "START: Preprocessing..\n",
      "\n",
      "Retrieving Essay Set: #4\n",
      "DataFrame Shape: (1772, 28)\n",
      "\n",
      "- PREPROCESSING ESSAY SETS -\n",
      "\n",
      "Getting Word Count..\n",
      "Getting Unique Word Count..\n",
      "Getting Sentence Count..\n",
      "Getting Average Word Length..\n",
      "POS Tagging..\n",
      "Getting Readability..\n",
      "Getting Grammatical Errors..\n",
      "Autocorrecting Essay..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\2340480230.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "\n",
      "- PREPROCESSING SOURCE ESSAY -\n",
      "\n",
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "PROCESS: Vectorization..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\674088542.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Training..\n",
      "\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 3\n",
      "Mean Absolute Error: 0.42910061414875644\n",
      "Mean Squared Error: 0.29122249410598244\n",
      "Root Mean Squared Error: 0.5396503443026628\n",
      "R2 score: 0.6662291540100667\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 3\n",
      "Overall Score: 0.6871079830183744\n",
      "\n",
      "\n",
      "\n",
      "START: Preprocessing..\n",
      "\n",
      "Retrieving Essay Set: #5\n",
      "DataFrame Shape: (1805, 28)\n",
      "\n",
      "- PREPROCESSING ESSAY SETS -\n",
      "\n",
      "Getting Word Count..\n",
      "Getting Unique Word Count..\n",
      "Getting Sentence Count..\n",
      "Getting Average Word Length..\n",
      "POS Tagging..\n",
      "Getting Readability..\n",
      "Getting Grammatical Errors..\n",
      "Autocorrecting Essay..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\2340480230.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "\n",
      "- PREPROCESSING SOURCE ESSAY -\n",
      "\n",
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "PROCESS: Vectorization..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\674088542.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Training..\n",
      "\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 3\n",
      "Mean Absolute Error: 0.38612736709320294\n",
      "Mean Squared Error: 0.24319996663649626\n",
      "Root Mean Squared Error: 0.49315308641079825\n",
      "R2 score: 0.7135677743700877\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 3\n",
      "Overall Score: 0.7178880151673541\n",
      "\n",
      "\n",
      "\n",
      "START: Preprocessing..\n",
      "\n",
      "Retrieving Essay Set: #6\n",
      "DataFrame Shape: (1800, 28)\n",
      "\n",
      "- PREPROCESSING ESSAY SETS -\n",
      "\n",
      "Getting Word Count..\n",
      "Getting Unique Word Count..\n",
      "Getting Sentence Count..\n",
      "Getting Average Word Length..\n",
      "POS Tagging..\n",
      "Getting Readability..\n",
      "Getting Grammatical Errors..\n",
      "Autocorrecting Essay..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\2340480230.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "\n",
      "- PREPROCESSING SOURCE ESSAY -\n",
      "\n",
      "Tokenizing..\n",
      "Removing Stop Words..\n",
      "Stemming Words..\n",
      "Detokenizing..\n",
      "\n",
      "- FINISHED -\n",
      "\n",
      "PROCESS: Vectorization..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_284\\674088542.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  source_essay['essay'] = source_essay['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
      "c:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:268: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Training..\n",
      "\n",
      "Getting Features\n",
      "Splitting Dataset\n",
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n",
      "Getting Predictions\n",
      "\n",
      "Results:\n",
      "Model 3\n",
      "Mean Absolute Error: 0.4274291782865379\n",
      "Mean Squared Error: 0.28191610012910334\n",
      "Root Mean Squared Error: 0.5309577197189088\n",
      "R2 score: 0.7193808971134049\n",
      "\n",
      "Cross Validation 10-Folds\n",
      "Model 3\n",
      "Overall Score: 0.7182948845210234\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sd_summaries = [(3, 10, 10000),\n",
    "            (4, 5, 10000), \n",
    "            (5, 10, 10000), \n",
    "            (6, 1, 1000)]\n",
    "\n",
    "sd_summary_scores = []\n",
    "\n",
    "for summary in sd_summaries:\n",
    "    print(\"START: Preprocessing..\\n\")\n",
    "    clean_df = preprocess_dataframe(summary[0])\n",
    "    source_df = preprocess_source_essay(summary[0])\n",
    "    \n",
    "    print(\"PROCESS: Vectorization..\\n\")\n",
    "    combined_df, svd = sd_vectorization_process(clean_df, source_df, summary[1], summary[2])\n",
    "    \n",
    "    print(\"PROCESS: Training..\\n\")\n",
    "    best_score, average_score = training_process(combined_df, svd)\n",
    "\n",
    "    sd_summary_scores.append([best_score, average_score])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d4804b05511593ff5d39042ce4458bba250db19b4b42c28c2a085011e4afa5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
