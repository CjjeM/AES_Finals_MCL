{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 243,
=======
   "execution_count": 5,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import language_tool_python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "\n",
    "language_tool = language_tool_python.LanguageTool('en-US')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 315,
=======
   "execution_count": 6,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12978, 28)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 315,
=======
     "execution_count": 6,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"training_set_rel3.xls\")\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 316,
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 28)\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"essay_set\"]==1]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
   "metadata": {},
   "outputs": [],
   "source": [
    "# essay structure\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def unique_word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    unique_words = set(words)\n",
    "\n",
    "    return len(unique_words)\n",
    "\n",
    "def sentence_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)\n",
    "\n",
    "def avg_word_len(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# get grammar errors\n",
    "def grammar_errors(essay):\n",
    "    errors = language_tool.check(essay)\n",
    "    return len(errors)\n",
    "\n",
    "# fix errors\n",
    "def autocorrect_essay(essay):\n",
    "    corrected_essay = language_tool.correct(essay)\n",
    "    return corrected_essay"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 317,
=======
   "execution_count": 10,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df, essay_set):\n",
    "    df = df[df[\"essay_set\"]==essay_set]\n",
    "    print(f\"Retrieving Essay Set #{essay_set}\")\n",
    "    print(f\"Dataframe shape: {df.shape}\")\n",
    "    clean_df = df[['essay', 'domain1_score']].copy()\n",
    "    clean_df = clean_df.rename(columns={'domain1_score': 'actual_score'})\n",
    "\n",
    "    # get essay structure\n",
    "    print(\"Getting Word Count\")\n",
    "    clean_df['word_count'] = clean_df['essay'].apply(word_count)\n",
    "    print(\"Getting Unique Word Count\")\n",
    "    clean_df['unique_word_count'] = clean_df['essay'].apply(unique_word_count)\n",
    "    print(\"Getting Sentence Count\")\n",
    "    clean_df['sentence_count'] = clean_df['essay'].apply(sentence_count)\n",
    "    print(\"Getting Average Word Length\")\n",
    "    clean_df['avg_word_len'] = clean_df['essay'].apply(avg_word_len)\n",
    "\n",
    "    # get grammatical errors\n",
    "    print(\"Getting Grammatical Errors\")\n",
    "    clean_df['grammar_errors'] = clean_df['essay'].apply(grammar_errors)\n",
    "    \n",
    "    # autocorrect errors\n",
    "    print(\"Autocorrecting Essay\")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(autocorrect_essay)\n",
    "\n",
    "    # preprocess essay for tokenization\n",
    "    print(\"Preprocess for tokenization\")\n",
    "    clean_df.reset_index(drop=True, inplace=True)\n",
    "    clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenization\n",
    "    print(\"Tokenization Start\")\n",
    "    tokenized_doc = clean_df['essay'].apply(lambda x: x.split())\n",
    "\n",
    "    # remove stop-words\n",
    "    print(\"Removing Stop Words\")\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    # stemming\n",
    "    print(\"Word Stemming\")\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "    # de-tokenization\n",
    "    print(\"Detokenize\")\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(clean_df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    clean_df['essay'] = detokenized_doc\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 247,
=======
   "execution_count": 11,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving Essay Set #1\n",
      "Dataframe shape: (1783, 28)\n",
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
<<<<<<< HEAD
      "Preprocess for tokenization\n",
      "Tokenization Start\n"
=======
      "Preprocess for tokenization\n"
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_22452\\1190138480.py:29: FutureWarning: The default value of regex will change from True to False in a future version.\n",
=======
      "C:\\Users\\Mark Anthony Mamauag\\AppData\\Local\\Temp\\ipykernel_10804\\3582771511.py:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
=======
      "Tokenization Start\n",
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
      "Removing Stop Words\n",
      "Word Stemming\n",
      "Detokenize\n"
     ]
    }
   ],
   "source": [
    "clean_df = preprocess_dataframe(df, 1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 318,
=======
   "execution_count": 12,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 7)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 318,
=======
     "execution_count": 12,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF Matrix Shape: (1783, 2500)\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer for the training data\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                        stop_words='english',\n",
    "                                        ngram_range = (1,3),\n",
    "                                        tokenizer = tokenizer.tokenize,\n",
    "                                        max_features=2500,\n",
    "                                        max_df=0.8,\n",
    "                                        min_df=5)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(clean_df[\"essay\"])\n",
    "print(f\"Train TFIDF Matrix Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# TFIDF to SVD\n",
    "svd_model = TruncatedSVD(n_components=100,\n",
    "                         n_iter=200,\n",
    "                         random_state=69)\n",
    "    \n",
    "svd = svd_model.fit_transform(tfidf_matrix)\n",
    "#normalized_svd = Normalizer(copy=False).fit_transform(svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>grammar_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear local newspap think effect comput peopl g...</td>\n",
       "      <td>8</td>\n",
       "      <td>386</td>\n",
       "      <td>173</td>\n",
       "      <td>16</td>\n",
       "      <td>4.237143</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear cap cap believ use comput benefit us mani...</td>\n",
       "      <td>9</td>\n",
       "      <td>464</td>\n",
       "      <td>205</td>\n",
       "      <td>20</td>\n",
       "      <td>4.312057</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear cap cap cap peopl use comput everyon agre...</td>\n",
       "      <td>7</td>\n",
       "      <td>313</td>\n",
       "      <td>160</td>\n",
       "      <td>14</td>\n",
       "      <td>4.342756</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear local newspap cap found mani expert say c...</td>\n",
       "      <td>10</td>\n",
       "      <td>611</td>\n",
       "      <td>260</td>\n",
       "      <td>27</td>\n",
       "      <td>4.813208</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dear locat know comput posit effect peopl comp...</td>\n",
       "      <td>8</td>\n",
       "      <td>517</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>4.334038</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  actual_score  \\\n",
       "0  dear local newspap think effect comput peopl g...             8   \n",
       "1  dear cap cap believ use comput benefit us mani...             9   \n",
       "2  dear cap cap cap peopl use comput everyon agre...             7   \n",
       "3  dear local newspap cap found mani expert say c...            10   \n",
       "4  dear locat know comput posit effect peopl comp...             8   \n",
       "\n",
       "   word_count  unique_word_count  sentence_count  avg_word_len  grammar_errors  \n",
       "0         386                173              16      4.237143              16  \n",
       "1         464                205              20      4.312057              25  \n",
       "2         313                160              14      4.342756              17  \n",
       "3         611                260              27      4.813208              29  \n",
       "4         517                210              30      4.334038              17  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_features = clean_df[['word_count', 'unique_word_count', 'sentence_count', 'avg_word_len', 'grammar_errors']]\n",
    "\n",
    "x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "y_features = clean_df['actual_score'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear regression, LogisticRegression, SVR\n",
    "\n",
    "print(\"Building Linear Regression Model\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building SVR Model\")\n",
    "svr_model = SVR()\n",
    "svr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Decision Tree Model\")\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Bayesian Regressor\")\n",
    "bayes_model = BayesianRidge()\n",
    "bayes_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building AdaBoost Regressor\")\n",
    "ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "ada_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Random Forest Regressor\")\n",
    "ran_model = RandomForestRegressor()\n",
    "ran_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Gradient Boosting Regressor\")\n",
    "grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "grad_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Logistic Regression Model\")\n",
    "log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "log_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [lr_model.predict(x_test),\n",
    "               svr_model.predict(x_test),\n",
    "               tree_model.predict(x_test),\n",
    "               bayes_model.predict(x_test),\n",
    "               ada_model.predict(x_test),\n",
    "               ran_model.predict(x_test),\n",
    "               grad_model.predict(x_test),\n",
    "               log_model.predict(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Mean Absolute Error: 0.6889305436083422\n",
      "Mean Squared Error: 0.7474858779700067\n",
      "Root Mean Squared Error: 0.8645726562701406\n",
      "R2 score: 0.7069125175010171\n",
      "\n",
      "Model 1\n",
      "Mean Absolute Error: 0.6457722480812125\n",
      "Mean Squared Error: 0.6485512670686159\n",
      "Root Mean Squared Error: 0.8053268076182587\n",
      "R2 score: 0.7457045494252224\n",
      "\n",
      "Model 2\n",
      "Mean Absolute Error: 0.8067226890756303\n",
      "Mean Squared Error: 1.2492997198879552\n",
      "Root Mean Squared Error: 1.1177207700888245\n",
      "R2 score: 0.5101524716653745\n",
      "\n",
      "Model 3\n",
      "Mean Absolute Error: 0.6650846036902245\n",
      "Mean Squared Error: 0.702677346952433\n",
      "Root Mean Squared Error: 0.8382585203577909\n",
      "R2 score: 0.7244818387918539\n",
      "\n",
      "Model 4\n",
      "Mean Absolute Error: 0.6657151413600464\n",
      "Mean Squared Error: 0.6532677118403866\n",
      "Root Mean Squared Error: 0.8082497830747538\n",
      "R2 score: 0.7438552423476654\n",
      "\n",
      "Model 5\n",
      "Mean Absolute Error: 0.6276750700280113\n",
      "Mean Squared Error: 0.6270369747899159\n",
      "Root Mean Squared Error: 0.7918566630330997\n",
      "R2 score: 0.7541402536272013\n",
      "\n",
      "Model 6\n",
      "Mean Absolute Error: 0.6230174968244881\n",
      "Mean Squared Error: 0.6390500759228234\n",
      "Root Mean Squared Error: 0.7994060769864233\n",
      "R2 score: 0.7494299444804767\n",
      "\n",
      "Model 7\n",
      "Mean Absolute Error: 0.803921568627451\n",
      "Mean Squared Error: 1.4873949579831933\n",
      "Root Mean Squared Error: 1.21958802797633\n",
      "R2 score: 0.41679587994240774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, pred in enumerate(predictions):\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_score = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"R2 score: {r_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chaldea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = [cross_val_score(lr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(svr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(tree_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(bayes_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ada_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ran_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(grad_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(log_model, x_train, y_train, cv=10).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Overall Score: 0.4369105863192447\n",
      "\n",
      "Model 1\n",
      "Overall Score: 0.33429916779771734\n",
      "\n",
      "Model 2\n",
      "Overall Score: 0.05267121184294262\n",
      "\n",
      "Model 3\n",
      "Overall Score: 0.5173990300022441\n",
      "\n",
      "Model 4\n",
      "Overall Score: 0.4783045102529061\n",
      "\n",
      "Model 5\n",
      "Overall Score: 0.5057560786388862\n",
      "\n",
      "Model 6\n",
      "Overall Score: 0.47809385649535924\n",
      "\n",
      "Model 7\n",
      "Overall Score: 0.2335753176043557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, score in enumerate(scores):\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Overall Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with LSA score"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 328,
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>grammar_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear local newspap think effect comput peopl g...</td>\n",
       "      <td>8</td>\n",
       "      <td>386</td>\n",
       "      <td>173</td>\n",
       "      <td>16</td>\n",
       "      <td>4.237143</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear cap cap believ use comput benefit us mani...</td>\n",
       "      <td>9</td>\n",
       "      <td>464</td>\n",
       "      <td>205</td>\n",
       "      <td>20</td>\n",
       "      <td>4.312057</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear cap cap cap peopl use comput everyon agre...</td>\n",
       "      <td>7</td>\n",
       "      <td>313</td>\n",
       "      <td>160</td>\n",
       "      <td>14</td>\n",
       "      <td>4.342756</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear local newspap cap found mani expert say c...</td>\n",
       "      <td>10</td>\n",
       "      <td>611</td>\n",
       "      <td>260</td>\n",
       "      <td>27</td>\n",
       "      <td>4.813208</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dear locat know comput posit effect peopl comp...</td>\n",
       "      <td>8</td>\n",
       "      <td>517</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>4.334038</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>dear cap cap sever reason way advanc technolog...</td>\n",
       "      <td>8</td>\n",
       "      <td>548</td>\n",
       "      <td>223</td>\n",
       "      <td>21</td>\n",
       "      <td>4.015717</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>adult kid spend much time comput well think su...</td>\n",
       "      <td>7</td>\n",
       "      <td>235</td>\n",
       "      <td>116</td>\n",
       "      <td>18</td>\n",
       "      <td>4.004673</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>opinion peopl comput home comput import thing ...</td>\n",
       "      <td>8</td>\n",
       "      <td>314</td>\n",
       "      <td>106</td>\n",
       "      <td>18</td>\n",
       "      <td>4.489865</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>dear reader think good bad use comput much</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>dear local newspap agre comput good societi wi...</td>\n",
       "      <td>7</td>\n",
       "      <td>235</td>\n",
       "      <td>129</td>\n",
       "      <td>18</td>\n",
       "      <td>4.069124</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1763 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay  actual_score  \\\n",
       "0     dear local newspap think effect comput peopl g...             8   \n",
       "1     dear cap cap believ use comput benefit us mani...             9   \n",
       "2     dear cap cap cap peopl use comput everyon agre...             7   \n",
       "3     dear local newspap cap found mani expert say c...            10   \n",
       "4     dear locat know comput posit effect peopl comp...             8   \n",
       "...                                                 ...           ...   \n",
       "1778  dear cap cap sever reason way advanc technolog...             8   \n",
       "1779  adult kid spend much time comput well think su...             7   \n",
       "1780  opinion peopl comput home comput import thing ...             8   \n",
       "1781         dear reader think good bad use comput much             2   \n",
       "1782  dear local newspap agre comput good societi wi...             7   \n",
       "\n",
       "      word_count  unique_word_count  sentence_count  avg_word_len  \\\n",
       "0            386                173              16      4.237143   \n",
       "1            464                205              20      4.312057   \n",
       "2            313                160              14      4.342756   \n",
       "3            611                260              27      4.813208   \n",
       "4            517                210              30      4.334038   \n",
       "...          ...                ...             ...           ...   \n",
       "1778         548                223              21      4.015717   \n",
       "1779         235                116              18      4.004673   \n",
       "1780         314                106              18      4.489865   \n",
       "1781          16                 14               1      3.733333   \n",
       "1782         235                129              18      4.069124   \n",
       "\n",
       "      grammar_errors  \n",
       "0                 16  \n",
       "1                 25  \n",
       "2                 17  \n",
       "3                 29  \n",
       "4                 17  \n",
       "...              ...  \n",
       "1778              33  \n",
       "1779              16  \n",
       "1780              15  \n",
       "1781               0  \n",
       "1782              20  \n",
       "\n",
       "[1763 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lsa = clean_df.copy()\n",
    "\n",
    "chosen_essay = df_lsa[df_lsa['actual_score'] >= 11]\n",
    "chosen_essay = chosen_essay.groupby('actual_score').sample(10, random_state=26)\n",
    "chosen_essay\n",
    "\n",
    "df_lsa = df_lsa.drop(index = chosen_essay.index)\n",
    "df_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
>>>>>>> 54d016ef06e57457559ee7c1178003c8197a1c5a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "df_lsa = clean_df.copy()\n",
    "largest_possible_score = df_lsa.nlargest(1, 'actual_score')['actual_score'].values[0]\n",
    "\n",
    "top_score = largest_possible_score - (largest_possible_score * 0.10)\n",
    "\n",
    "chosen_essay = df_lsa[df_lsa['actual_score'] >= top_score]\n",
    "print(len(chosen_essay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>grammar_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dear organ comput blink life imag blond hair g...</td>\n",
       "      <td>12</td>\n",
       "      <td>605</td>\n",
       "      <td>260</td>\n",
       "      <td>35</td>\n",
       "      <td>4.799625</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dear local newspap heard mani peopl think comp...</td>\n",
       "      <td>11</td>\n",
       "      <td>673</td>\n",
       "      <td>261</td>\n",
       "      <td>39</td>\n",
       "      <td>4.397569</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dear organ cap brought attent peopl feel compu...</td>\n",
       "      <td>12</td>\n",
       "      <td>554</td>\n",
       "      <td>247</td>\n",
       "      <td>31</td>\n",
       "      <td>4.276986</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ever time complet fli comput go time pm sudden...</td>\n",
       "      <td>11</td>\n",
       "      <td>626</td>\n",
       "      <td>233</td>\n",
       "      <td>33</td>\n",
       "      <td>3.959075</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>dear cap cap cap life without comput would qui...</td>\n",
       "      <td>12</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>28</td>\n",
       "      <td>4.764706</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>dear local newspap cap know comput teach mani ...</td>\n",
       "      <td>11</td>\n",
       "      <td>795</td>\n",
       "      <td>280</td>\n",
       "      <td>51</td>\n",
       "      <td>4.087218</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>reader locat cap well inform benefit comput so...</td>\n",
       "      <td>11</td>\n",
       "      <td>609</td>\n",
       "      <td>283</td>\n",
       "      <td>22</td>\n",
       "      <td>4.755027</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>dear organ new digit age comput beam advanc an...</td>\n",
       "      <td>11</td>\n",
       "      <td>484</td>\n",
       "      <td>229</td>\n",
       "      <td>34</td>\n",
       "      <td>4.538647</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>dear newspap cap firmli believ comput benefici...</td>\n",
       "      <td>11</td>\n",
       "      <td>422</td>\n",
       "      <td>218</td>\n",
       "      <td>30</td>\n",
       "      <td>4.574526</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>cap judgment come experi experi come bad judgm...</td>\n",
       "      <td>11</td>\n",
       "      <td>652</td>\n",
       "      <td>277</td>\n",
       "      <td>36</td>\n",
       "      <td>4.613309</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>dear local newspap cap cap technolog advanc li...</td>\n",
       "      <td>12</td>\n",
       "      <td>544</td>\n",
       "      <td>227</td>\n",
       "      <td>28</td>\n",
       "      <td>4.747153</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>dear organ understand idea advanc technolog be...</td>\n",
       "      <td>11</td>\n",
       "      <td>509</td>\n",
       "      <td>224</td>\n",
       "      <td>25</td>\n",
       "      <td>4.868056</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>hour hour game onlin post blog social network ...</td>\n",
       "      <td>12</td>\n",
       "      <td>670</td>\n",
       "      <td>312</td>\n",
       "      <td>39</td>\n",
       "      <td>4.574043</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>dear local newspap believ comput extrem use to...</td>\n",
       "      <td>12</td>\n",
       "      <td>507</td>\n",
       "      <td>211</td>\n",
       "      <td>33</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>sinc comput invent coloss chang way societi fu...</td>\n",
       "      <td>12</td>\n",
       "      <td>608</td>\n",
       "      <td>263</td>\n",
       "      <td>29</td>\n",
       "      <td>4.528545</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>month time use comput one day lot right think ...</td>\n",
       "      <td>11</td>\n",
       "      <td>562</td>\n",
       "      <td>205</td>\n",
       "      <td>39</td>\n",
       "      <td>4.528571</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>organ cap click clack sound organ comput key h...</td>\n",
       "      <td>11</td>\n",
       "      <td>873</td>\n",
       "      <td>310</td>\n",
       "      <td>39</td>\n",
       "      <td>4.456609</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>comput cap far come comput hold mani opportun ...</td>\n",
       "      <td>12</td>\n",
       "      <td>620</td>\n",
       "      <td>259</td>\n",
       "      <td>43</td>\n",
       "      <td>4.178309</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>late technolog buzz hit world ever look home s...</td>\n",
       "      <td>12</td>\n",
       "      <td>648</td>\n",
       "      <td>291</td>\n",
       "      <td>36</td>\n",
       "      <td>4.560297</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>dear organ cap would life effect comput strong...</td>\n",
       "      <td>11</td>\n",
       "      <td>688</td>\n",
       "      <td>224</td>\n",
       "      <td>34</td>\n",
       "      <td>3.865719</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 essay  actual_score  \\\n",
       "15   dear organ comput blink life imag blond hair g...            12   \n",
       "23   dear local newspap heard mani peopl think comp...            11   \n",
       "36   dear organ cap brought attent peopl feel compu...            12   \n",
       "47   ever time complet fli comput go time pm sudden...            11   \n",
       "69   dear cap cap cap life without comput would qui...            12   \n",
       "91   dear local newspap cap know comput teach mani ...            11   \n",
       "92   reader locat cap well inform benefit comput so...            11   \n",
       "97   dear organ new digit age comput beam advanc an...            11   \n",
       "105  dear newspap cap firmli believ comput benefici...            11   \n",
       "110  cap judgment come experi experi come bad judgm...            11   \n",
       "117  dear local newspap cap cap technolog advanc li...            12   \n",
       "131  dear organ understand idea advanc technolog be...            11   \n",
       "146  hour hour game onlin post blog social network ...            12   \n",
       "153  dear local newspap believ comput extrem use to...            12   \n",
       "161  sinc comput invent coloss chang way societi fu...            12   \n",
       "191  month time use comput one day lot right think ...            11   \n",
       "198  organ cap click clack sound organ comput key h...            11   \n",
       "224  comput cap far come comput hold mani opportun ...            12   \n",
       "231  late technolog buzz hit world ever look home s...            12   \n",
       "242  dear organ cap would life effect comput strong...            11   \n",
       "\n",
       "     word_count  unique_word_count  sentence_count  avg_word_len  \\\n",
       "15          605                260              35      4.799625   \n",
       "23          673                261              39      4.397569   \n",
       "36          554                247              31      4.276986   \n",
       "47          626                233              33      3.959075   \n",
       "69          510                221              28      4.764706   \n",
       "91          795                280              51      4.087218   \n",
       "92          609                283              22      4.755027   \n",
       "97          484                229              34      4.538647   \n",
       "105         422                218              30      4.574526   \n",
       "110         652                277              36      4.613309   \n",
       "117         544                227              28      4.747153   \n",
       "131         509                224              25      4.868056   \n",
       "146         670                312              39      4.574043   \n",
       "153         507                211              33      4.500000   \n",
       "161         608                263              29      4.528545   \n",
       "191         562                205              39      4.528571   \n",
       "198         873                310              39      4.456609   \n",
       "224         620                259              43      4.178309   \n",
       "231         648                291              36      4.560297   \n",
       "242         688                224              34      3.865719   \n",
       "\n",
       "     grammar_errors  \n",
       "15               21  \n",
       "23               12  \n",
       "36               32  \n",
       "47                6  \n",
       "69               14  \n",
       "91               29  \n",
       "92               15  \n",
       "97               17  \n",
       "105              10  \n",
       "110              13  \n",
       "117              10  \n",
       "131              13  \n",
       "146              15  \n",
       "153               6  \n",
       "161              23  \n",
       "191              28  \n",
       "198              33  \n",
       "224              22  \n",
       "231               8  \n",
       "242              20  "
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_essay.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsa = clean_df.copy()\n",
    "largest_possible_score = df_lsa.nlargest(1, 'actual_score')['actual_score'].values[0]\n",
    "\n",
    "top_score = largest_possible_score - (largest_possible_score * 0.10)\n",
    "\n",
    "chosen_essay = df_lsa[df_lsa['actual_score'] >= top_score]\n",
    "chosen_essay = chosen_essay.groupby('actual_score').sample(5, random_state=26)\n",
    "\n",
    "df_lsa = df_lsa.drop(index = chosen_essay.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorizer for lsa similarity\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf_lsa_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                        stop_words='english',\n",
    "                                        ngram_range = (1,3),\n",
    "                                        tokenizer = tokenizer.tokenize,\n",
    "                                        max_features=1000,\n",
    "                                        max_df=0.8,\n",
    "                                        min_df=0.1)\n",
    "\n",
    "tfidf_lsa_matrix = tfidf_lsa_vectorizer.fit_transform(chosen_essay[\"essay\"])\n",
    "\n",
    "# TFIDF to SVD\n",
    "svd_lsa_model = TruncatedSVD(n_components=100,\n",
    "                        n_iter=200,\n",
    "                        random_state=69)\n",
    "    \n",
    "svd_lsa = svd_lsa_model.fit_transform(tfidf_lsa_matrix)\n",
    "#normalized_svd = Normalizer(copy=False).fit_transform(svd_lsa)\n",
    "\n",
    "def lsa_score(essay):\n",
    "    essay_matrix = tfidf_lsa_vectorizer.transform([essay])\n",
    "    essay_svd = svd_lsa_model.transform(essay_matrix)\n",
    "    #normalized_essay_svd = Normalizer(copy=False).fit_transform(essay_svd)\n",
    "\n",
    "    # Compare current essay to the top 10% scored essay\n",
    "    similarities = cosine_similarity(essay_svd, svd_lsa)[0]\n",
    "\n",
    "    return similarities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsa['lsa_score'] = df_lsa['essay'].apply(lsa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF Matrix Shape: (1773, 2500)\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer for the training data\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                   stop_words='english',\n",
    "                                   ngram_range = (1,3),\n",
    "                                   tokenizer = tokenizer.tokenize,\n",
    "                                   max_features=2500,\n",
    "                                   max_df=0.8,\n",
    "                                   min_df=5)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_lsa[\"essay\"])\n",
    "print(f\"Train TFIDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# TFIDF to SVD\n",
    "svd_model = TruncatedSVD(n_components=100,\n",
    "                         n_iter=200,\n",
    "                         random_state=69)\n",
    "    \n",
    "svd = svd_model.fit_transform(tfidf_matrix)\n",
    "#normalized_svd = Normalizer(copy=False).fit_transform(svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_features = df_lsa[['word_count', 'unique_word_count', 'sentence_count', 'avg_word_len', 'grammar_errors', 'lsa_score']]\n",
    "\n",
    "x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "y_features = df_lsa['actual_score'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear regression, LogisticRegression, SVR\n",
    "\n",
    "print(\"Building Linear Regression Model\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building SVR Model\")\n",
    "svr_model = SVR()\n",
    "svr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Decision Tree Model\")\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Bayesian Regressor\")\n",
    "bayes_model = BayesianRidge()\n",
    "bayes_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building AdaBoost Regressor\")\n",
    "ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "ada_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Random Forest Regressor\")\n",
    "ran_model = RandomForestRegressor()\n",
    "ran_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Gradient Boosting Regressor\")\n",
    "grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "grad_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Logistic Regression Model\")\n",
    "log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "log_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [lr_model.predict(x_test),\n",
    "               svr_model.predict(x_test),\n",
    "               tree_model.predict(x_test),\n",
    "               bayes_model.predict(x_test),\n",
    "               ada_model.predict(x_test),\n",
    "               ran_model.predict(x_test),\n",
    "               grad_model.predict(x_test),\n",
    "               log_model.predict(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Mean Absolute Error: 0.6836835564802185\n",
      "Mean Squared Error: 0.7461688996812411\n",
      "Root Mean Squared Error: 0.8638106850932333\n",
      "R2 score: 0.6354546330650875\n",
      "\n",
      "Model 1\n",
      "Mean Absolute Error: 0.640806806560233\n",
      "Mean Squared Error: 0.6563445230799838\n",
      "Root Mean Squared Error: 0.810150926111909\n",
      "R2 score: 0.6793388801059299\n",
      "\n",
      "Model 2\n",
      "Mean Absolute Error: 0.9014084507042254\n",
      "Mean Squared Error: 1.504225352112676\n",
      "Root Mean Squared Error: 1.226468651092508\n",
      "R2 score: 0.2651015297301069\n",
      "\n",
      "Model 3\n",
      "Mean Absolute Error: 0.6542275200340179\n",
      "Mean Squared Error: 0.6971600313182189\n",
      "Root Mean Squared Error: 0.8349610956914214\n",
      "R2 score: 0.6593982146162551\n",
      "\n",
      "Model 4\n",
      "Mean Absolute Error: 0.6664738178684437\n",
      "Mean Squared Error: 0.6911967423280344\n",
      "Root Mean Squared Error: 0.8313824284455587\n",
      "R2 score: 0.6623116158234006\n",
      "\n",
      "Model 5\n",
      "Mean Absolute Error: 0.6319154929577464\n",
      "Mean Squared Error: 0.6609332394366197\n",
      "Root Mean Squared Error: 0.8129780067361108\n",
      "R2 score: 0.6770970347426285\n",
      "\n",
      "Model 6\n",
      "Mean Absolute Error: 0.6368102971804737\n",
      "Mean Squared Error: 0.6664257640848747\n",
      "Root Mean Squared Error: 0.8163490454976197\n",
      "R2 score: 0.6744136283259947\n",
      "\n",
      "Model 7\n",
      "Mean Absolute Error: 0.7436619718309859\n",
      "Mean Squared Error: 1.2394366197183098\n",
      "Root Mean Squared Error: 1.1132998786123665\n",
      "R2 score: 0.39446567992742887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, pred in enumerate(predictions):\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_score = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"R2 score: {r_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = [cross_val_score(lr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(svr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(tree_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(bayes_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ada_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ran_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(grad_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(log_model, x_train, y_train, cv=10).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Overall Score: 0.6762906202765445\n",
      "\n",
      "Model 1\n",
      "Overall Score: 0.7204772810151014\n",
      "\n",
      "Model 2\n",
      "Overall Score: 0.4326863192518579\n",
      "\n",
      "Model 3\n",
      "Overall Score: 0.7001464221689246\n",
      "\n",
      "Model 4\n",
      "Overall Score: 0.702954700103647\n",
      "\n",
      "Model 5\n",
      "Overall Score: 0.7219493410579478\n",
      "\n",
      "Model 6\n",
      "Overall Score: 0.7071275213281665\n",
      "\n",
      "Model 7\n",
      "Overall Score: 0.48723404255319147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, score in enumerate(scores):\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Overall Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "### Best Performing Model for Essay Set 1\n",
    "**Model 5 (Random Forest Regressor, no LSA score)**<br>\n",
    "Mean Absolute Error: 0.6238935574229693<br>\n",
    "Mean Squared Error: 0.6290591036414567<br>\n",
    "Root Mean Squared Error: 0.7931324628594247<br>\n",
    "R2 score: 0.7533473815852623<br><br>\n",
    "\n",
    "**Model 5 (Random Forest Regressor, no LSA score, 10-Fold Validation)**<br>\n",
    "Overall Score: 0.7219474896299766<br><br>\n",
    "\n",
    "**Model 5 (Random Forest Regressor, with LSA score)**<br>\n",
    "Mean Absolute Error: 0.563399433427762<br>\n",
    "Mean Squared Error: 0.5285376770538244<br>\n",
    "Root Mean Squared Error: 0.727005967687903<br>\n",
    "R2 score: 0.7660819899557456<br><br>\n",
    "\n",
    "**Model 5 (Random Forest Regressor, with LSA score, 10-Fold Validation)**<br>\n",
    "Overall Score: 0.7126748931617651<br>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d4804b05511593ff5d39042ce4458bba250db19b4b42c28c2a085011e4afa5b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d888b7e27d9d4f8fccada6ae7e4260d620cc5227dee391d6d7538000bf76028"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
