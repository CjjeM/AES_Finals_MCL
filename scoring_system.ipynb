{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import language_tool_python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "\n",
    "language_tool = language_tool_python.LanguageTool('en-US')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12978, 28)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"training_set_rel3.xls\")\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 28)\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"essay_set\"]==1]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essay structure\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "def unique_word_count(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    unique_words = set(words)\n",
    "\n",
    "    return len(unique_words)\n",
    "\n",
    "def sentence_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)\n",
    "\n",
    "def avg_word_len(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# get grammar errors\n",
    "def grammar_errors(essay):\n",
    "    errors = language_tool.check(essay)\n",
    "    return len(errors)\n",
    "\n",
    "# fix errors\n",
    "def autocorrect_essay(essay):\n",
    "    corrected_essay = language_tool.correct(essay)\n",
    "    return corrected_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    clean_df = df[['essay', 'domain1_score']].copy()\n",
    "    clean_df = clean_df.rename(columns={'domain1_score': 'actual_score'})\n",
    "\n",
    "    # get essay structure\n",
    "    print(\"Getting Word Count\")\n",
    "    clean_df['word_count'] = clean_df['essay'].apply(word_count)\n",
    "    print(\"Getting Unique Word Count\")\n",
    "    clean_df['unique_word_count'] = clean_df['essay'].apply(unique_word_count)\n",
    "    print(\"Getting Sentence Count\")\n",
    "    clean_df['sentence_count'] = clean_df['essay'].apply(sentence_count)\n",
    "    print(\"Getting Average Word Length\")\n",
    "    clean_df['avg_word_len'] = clean_df['essay'].apply(avg_word_len)\n",
    "\n",
    "    # get grammatical errors\n",
    "    print(\"Getting Grammatical Errors\")\n",
    "    clean_df['grammar_errors'] = clean_df['essay'].apply(grammar_errors)\n",
    "    \n",
    "    # autocorrect errors\n",
    "    print(\"Autocorrecting Essay\")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(autocorrect_essay)\n",
    "\n",
    "    # preprocess essay for tokenization\n",
    "    print(\"Preprocess for tokenization\")\n",
    "    clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    clean_df['essay'] = clean_df['essay'].apply(lambda x: x.lower())\n",
    "\n",
    "    # tokenization\n",
    "    print(\"Tokenization Start\")\n",
    "    tokenized_doc = clean_df['essay'].apply(lambda x: x.split())\n",
    "\n",
    "    # remove stop-words\n",
    "    print(\"Removing Stop Words\")\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    # stemming\n",
    "    print(\"Word Stemming\")\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [porter_stemmer.stem(item) for item in x])\n",
    "\n",
    "    # de-tokenization\n",
    "    print(\"Detokenize\")\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(clean_df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    clean_df['essay'] = detokenized_doc\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Word Count\n",
      "Getting Unique Word Count\n",
      "Getting Sentence Count\n",
      "Getting Average Word Length\n",
      "Getting Grammatical Errors\n",
      "Autocorrecting Essay\n",
      "Preprocess for tokenization\n",
      "Tokenization Start\n",
      "Removing Stop Words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaldea\\AppData\\Local\\Temp\\ipykernel_1512\\3582771511.py:25: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  clean_df['essay'] = clean_df['essay'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming\n",
      "Detokenize\n"
     ]
    }
   ],
   "source": [
    "clean_df = preprocess_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 7)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF Matrix Shape: (1783, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer for the training data\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                        stop_words='english',\n",
    "                                        ngram_range = (1,3),\n",
    "                                        tokenizer = tokenizer.tokenize,\n",
    "                                        max_features=10000,\n",
    "                                        max_df=0.8,\n",
    "                                        min_df=5)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(clean_df[\"essay\"])\n",
    "print(f\"Train TFIDF Matrix Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# TFIDF to SVD\n",
    "svd_model = TruncatedSVD(n_components=100,\n",
    "                         n_iter=200,\n",
    "                         random_state=69)\n",
    "    \n",
    "svd = svd_model.fit_transform(tfidf_matrix)\n",
    "#normalized_svd = Normalizer(copy=False).fit_transform(svd)\n",
    "print(type(svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>grammar_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear local newspap think effect comput peopl g...</td>\n",
       "      <td>8</td>\n",
       "      <td>386</td>\n",
       "      <td>173</td>\n",
       "      <td>16</td>\n",
       "      <td>4.237143</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear cap cap believ use comput benefit us mani...</td>\n",
       "      <td>9</td>\n",
       "      <td>464</td>\n",
       "      <td>205</td>\n",
       "      <td>20</td>\n",
       "      <td>4.312057</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear cap cap cap peopl use comput everyon agre...</td>\n",
       "      <td>7</td>\n",
       "      <td>313</td>\n",
       "      <td>160</td>\n",
       "      <td>14</td>\n",
       "      <td>4.342756</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear local newspap cap found mani expert say c...</td>\n",
       "      <td>10</td>\n",
       "      <td>611</td>\n",
       "      <td>260</td>\n",
       "      <td>27</td>\n",
       "      <td>4.813208</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dear locat know comput posit effect peopl comp...</td>\n",
       "      <td>8</td>\n",
       "      <td>517</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>4.334038</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay  actual_score  \\\n",
       "0  dear local newspap think effect comput peopl g...             8   \n",
       "1  dear cap cap believ use comput benefit us mani...             9   \n",
       "2  dear cap cap cap peopl use comput everyon agre...             7   \n",
       "3  dear local newspap cap found mani expert say c...            10   \n",
       "4  dear locat know comput posit effect peopl comp...             8   \n",
       "\n",
       "   word_count  unique_word_count  sentence_count  avg_word_len  grammar_errors  \n",
       "0         386                173              16      4.237143              16  \n",
       "1         464                205              20      4.312057              25  \n",
       "2         313                160              14      4.342756              17  \n",
       "3         611                260              27      4.813208              29  \n",
       "4         517                210              30      4.334038              17  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_features = clean_df[['word_count', 'unique_word_count', 'sentence_count', 'avg_word_len', 'grammar_errors']]\n",
    "\n",
    "x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "y_features = clean_df['actual_score'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear regression, LogisticRegression, SVR\n",
    "\n",
    "print(\"Building Linear Regression Model\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building SVR Model\")\n",
    "svr_model = SVR()\n",
    "svr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Decision Tree Model\")\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Bayesian Regressor\")\n",
    "bayes_model = BayesianRidge()\n",
    "bayes_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building AdaBoost Regressor\")\n",
    "ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "ada_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Random Forest Regressor\")\n",
    "ran_model = RandomForestRegressor()\n",
    "ran_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Gradient Boosting Regressor\")\n",
    "grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "grad_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Logistic Regression Model\")\n",
    "log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "log_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [lr_model.predict(x_test),\n",
    "               svr_model.predict(x_test),\n",
    "               tree_model.predict(x_test),\n",
    "               bayes_model.predict(x_test),\n",
    "               ada_model.predict(x_test),\n",
    "               ran_model.predict(x_test),\n",
    "               grad_model.predict(x_test),\n",
    "               log_model.predict(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Mean Absolute Error: 0.6962940315898762\n",
      "Mean Squared Error: 0.7564628276790326\n",
      "Root Mean Squared Error: 0.8697487152500041\n",
      "R2 score: 0.7033926762996239\n",
      "\n",
      "Model 1\n",
      "Mean Absolute Error: 0.6457503899350246\n",
      "Mean Squared Error: 0.6485110830277652\n",
      "Root Mean Squared Error: 0.8053018583287668\n",
      "R2 score: 0.7457203054946233\n",
      "\n",
      "Model 2\n",
      "Mean Absolute Error: 0.8151260504201681\n",
      "Mean Squared Error: 1.3025210084033614\n",
      "Root Mean Squared Error: 1.1412804249628403\n",
      "R2 score: 0.48928452763318186\n",
      "\n",
      "Model 3\n",
      "Mean Absolute Error: 0.6653825727816108\n",
      "Mean Squared Error: 0.7034018230485286\n",
      "Root Mean Squared Error: 0.8386905406933648\n",
      "R2 score: 0.7241977733915657\n",
      "\n",
      "Model 4\n",
      "Mean Absolute Error: 0.6806680819693842\n",
      "Mean Squared Error: 0.6746980830037042\n",
      "Root Mean Squared Error: 0.8214000748744208\n",
      "R2 score: 0.7354524434207704\n",
      "\n",
      "Model 5\n",
      "Mean Absolute Error: 0.6171148459383754\n",
      "Mean Squared Error: 0.6293649859943977\n",
      "Root Mean Squared Error: 0.7933252712440199\n",
      "R2 score: 0.7532274458227193\n",
      "\n",
      "Model 6\n",
      "Mean Absolute Error: 0.6522712370556358\n",
      "Mean Squared Error: 0.6957773751074408\n",
      "Root Mean Squared Error: 0.834132708330899\n",
      "R2 score: 0.7271873017804721\n",
      "\n",
      "Model 7\n",
      "Mean Absolute Error: 0.8067226890756303\n",
      "Mean Squared Error: 1.4901960784313726\n",
      "Root Mean Squared Error: 1.2207358757861475\n",
      "R2 score: 0.41569756709860806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, pred in enumerate(predictions):\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_score = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"R2 score: {r_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chaldea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = [cross_val_score(lr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(svr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(tree_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(bayes_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ada_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ran_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(grad_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(log_model, x_train, y_train, cv=10).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Overall Score: 0.6846740218545614\n",
      "\n",
      "Model 1\n",
      "Overall Score: 0.717342881527865\n",
      "\n",
      "Model 2\n",
      "Overall Score: 0.41126713856826014\n",
      "\n",
      "Model 3\n",
      "Overall Score: 0.7026016271830992\n",
      "\n",
      "Model 4\n",
      "Overall Score: 0.6964191507053006\n",
      "\n",
      "Model 5\n",
      "Overall Score: 0.7224860731647178\n",
      "\n",
      "Model 6\n",
      "Overall Score: 0.713184690037685\n",
      "\n",
      "Model 7\n",
      "Overall Score: 0.47543090712104796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, score in enumerate(scores):\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Overall Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with LSA score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsa = clean_df.copy()\n",
    "\n",
    "chosen_essay = df_lsa[df_lsa['actual_score'] >= 11]\n",
    "chosen_essay = chosen_essay.groupby('actual_score').sample(10, random_state=26)\n",
    "\n",
    "df_lsa = df_lsa.drop(index = chosen_essay.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF Matrix Shape: (20, 250)\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer for lsa similarity\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf_lsa_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                        stop_words='english',\n",
    "                                        ngram_range = (1,3),\n",
    "                                        tokenizer = tokenizer.tokenize,\n",
    "                                        max_features=250,\n",
    "                                        max_df=0.8,\n",
    "                                        min_df=3)\n",
    "\n",
    "tfidf_lsa_matrix = tfidf_lsa_vectorizer.fit_transform(chosen_essay[\"essay\"])\n",
    "print(f\"Train TFIDF Matrix Shape: {tfidf_lsa_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF to SVD\n",
    "svd_lsa_model = TruncatedSVD(n_components=100,\n",
    "                         n_iter=200,\n",
    "                         random_state=69)\n",
    "    \n",
    "svd_lsa = svd_lsa_model.fit_transform(tfidf_lsa_matrix)\n",
    "normalized_svd = Normalizer(copy=False).fit_transform(svd_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_score(essay):\n",
    "    essay_matrix = tfidf_lsa_vectorizer.transform([essay])\n",
    "\n",
    "    essay_svd = svd_lsa_model.transform(essay_matrix)\n",
    "    normalized_essay_svd = Normalizer(copy=False).fit_transform(essay_svd)\n",
    "\n",
    "    similarities = cosine_similarity(normalized_svd, normalized_essay_svd).max()\n",
    "\n",
    "    return similarities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>actual_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>grammar_errors</th>\n",
       "      <th>lsa_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear local newspap think effect comput peopl g...</td>\n",
       "      <td>8</td>\n",
       "      <td>386</td>\n",
       "      <td>173</td>\n",
       "      <td>16</td>\n",
       "      <td>4.237143</td>\n",
       "      <td>16</td>\n",
       "      <td>0.645607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear cap cap believ use comput benefit us mani...</td>\n",
       "      <td>9</td>\n",
       "      <td>464</td>\n",
       "      <td>205</td>\n",
       "      <td>20</td>\n",
       "      <td>4.312057</td>\n",
       "      <td>25</td>\n",
       "      <td>0.746648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear cap cap cap peopl use comput everyon agre...</td>\n",
       "      <td>7</td>\n",
       "      <td>313</td>\n",
       "      <td>160</td>\n",
       "      <td>14</td>\n",
       "      <td>4.342756</td>\n",
       "      <td>17</td>\n",
       "      <td>0.676962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear local newspap cap found mani expert say c...</td>\n",
       "      <td>10</td>\n",
       "      <td>611</td>\n",
       "      <td>260</td>\n",
       "      <td>27</td>\n",
       "      <td>4.813208</td>\n",
       "      <td>29</td>\n",
       "      <td>0.729032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dear locat know comput posit effect peopl comp...</td>\n",
       "      <td>8</td>\n",
       "      <td>517</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>4.334038</td>\n",
       "      <td>17</td>\n",
       "      <td>0.728624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dear locat think comput neg effect us mani peo...</td>\n",
       "      <td>8</td>\n",
       "      <td>274</td>\n",
       "      <td>135</td>\n",
       "      <td>15</td>\n",
       "      <td>4.052632</td>\n",
       "      <td>17</td>\n",
       "      <td>0.722233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>know peopl day depend comput safeti natur educ...</td>\n",
       "      <td>10</td>\n",
       "      <td>580</td>\n",
       "      <td>231</td>\n",
       "      <td>30</td>\n",
       "      <td>4.385827</td>\n",
       "      <td>6</td>\n",
       "      <td>0.664469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>percent peopl agre comput make life less compl...</td>\n",
       "      <td>10</td>\n",
       "      <td>556</td>\n",
       "      <td>223</td>\n",
       "      <td>39</td>\n",
       "      <td>4.242126</td>\n",
       "      <td>7</td>\n",
       "      <td>0.930333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dear reader organ dramat effect human life cha...</td>\n",
       "      <td>9</td>\n",
       "      <td>512</td>\n",
       "      <td>224</td>\n",
       "      <td>35</td>\n",
       "      <td>4.190687</td>\n",
       "      <td>13</td>\n",
       "      <td>0.721024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>locat technolog comput say comput good societi...</td>\n",
       "      <td>9</td>\n",
       "      <td>561</td>\n",
       "      <td>220</td>\n",
       "      <td>26</td>\n",
       "      <td>3.982659</td>\n",
       "      <td>20</td>\n",
       "      <td>0.624251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dear locat cap peopl acknowledg great advanc c...</td>\n",
       "      <td>8</td>\n",
       "      <td>373</td>\n",
       "      <td>214</td>\n",
       "      <td>22</td>\n",
       "      <td>4.812121</td>\n",
       "      <td>5</td>\n",
       "      <td>0.799711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dear cap cap feel comput take away peopl life ...</td>\n",
       "      <td>8</td>\n",
       "      <td>435</td>\n",
       "      <td>166</td>\n",
       "      <td>25</td>\n",
       "      <td>4.361596</td>\n",
       "      <td>49</td>\n",
       "      <td>0.783211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dear local newspap red argument comput think p...</td>\n",
       "      <td>7</td>\n",
       "      <td>211</td>\n",
       "      <td>122</td>\n",
       "      <td>6</td>\n",
       "      <td>3.906863</td>\n",
       "      <td>27</td>\n",
       "      <td>0.578172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>three detail newspap articl one state opinion ...</td>\n",
       "      <td>6</td>\n",
       "      <td>332</td>\n",
       "      <td>134</td>\n",
       "      <td>25</td>\n",
       "      <td>4.188925</td>\n",
       "      <td>51</td>\n",
       "      <td>0.687472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dear world today everyon use comput comput pos...</td>\n",
       "      <td>6</td>\n",
       "      <td>197</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>4.485876</td>\n",
       "      <td>11</td>\n",
       "      <td>0.702640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dear organ comput blink life imag blond hair g...</td>\n",
       "      <td>12</td>\n",
       "      <td>605</td>\n",
       "      <td>260</td>\n",
       "      <td>35</td>\n",
       "      <td>4.799625</td>\n",
       "      <td>21</td>\n",
       "      <td>0.866593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dear local newspap believ comput neg effect pe...</td>\n",
       "      <td>8</td>\n",
       "      <td>385</td>\n",
       "      <td>149</td>\n",
       "      <td>18</td>\n",
       "      <td>4.092219</td>\n",
       "      <td>12</td>\n",
       "      <td>0.693132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dear local newspap must admit expert certainli...</td>\n",
       "      <td>8</td>\n",
       "      <td>420</td>\n",
       "      <td>177</td>\n",
       "      <td>15</td>\n",
       "      <td>4.112000</td>\n",
       "      <td>16</td>\n",
       "      <td>0.852123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>segr weigh evanesc tnachnolag evanesc tnachnol...</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>4.409091</td>\n",
       "      <td>34</td>\n",
       "      <td>0.628601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>well comput good bad thing cap real see cap co...</td>\n",
       "      <td>6</td>\n",
       "      <td>181</td>\n",
       "      <td>97</td>\n",
       "      <td>11</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.752274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                essay  actual_score  \\\n",
       "0   dear local newspap think effect comput peopl g...             8   \n",
       "1   dear cap cap believ use comput benefit us mani...             9   \n",
       "2   dear cap cap cap peopl use comput everyon agre...             7   \n",
       "3   dear local newspap cap found mani expert say c...            10   \n",
       "4   dear locat know comput posit effect peopl comp...             8   \n",
       "5   dear locat think comput neg effect us mani peo...             8   \n",
       "6   know peopl day depend comput safeti natur educ...            10   \n",
       "7   percent peopl agre comput make life less compl...            10   \n",
       "8   dear reader organ dramat effect human life cha...             9   \n",
       "9   locat technolog comput say comput good societi...             9   \n",
       "10  dear locat cap peopl acknowledg great advanc c...             8   \n",
       "11  dear cap cap feel comput take away peopl life ...             8   \n",
       "12  dear local newspap red argument comput think p...             7   \n",
       "13  three detail newspap articl one state opinion ...             6   \n",
       "14  dear world today everyon use comput comput pos...             6   \n",
       "15  dear organ comput blink life imag blond hair g...            12   \n",
       "16  dear local newspap believ comput neg effect pe...             8   \n",
       "17  dear local newspap must admit expert certainli...             8   \n",
       "18  segr weigh evanesc tnachnolag evanesc tnachnol...             4   \n",
       "19  well comput good bad thing cap real see cap co...             6   \n",
       "\n",
       "    word_count  unique_word_count  sentence_count  avg_word_len  \\\n",
       "0          386                173              16      4.237143   \n",
       "1          464                205              20      4.312057   \n",
       "2          313                160              14      4.342756   \n",
       "3          611                260              27      4.813208   \n",
       "4          517                210              30      4.334038   \n",
       "5          274                135              15      4.052632   \n",
       "6          580                231              30      4.385827   \n",
       "7          556                223              39      4.242126   \n",
       "8          512                224              35      4.190687   \n",
       "9          561                220              26      3.982659   \n",
       "10         373                214              22      4.812121   \n",
       "11         435                166              25      4.361596   \n",
       "12         211                122               6      3.906863   \n",
       "13         332                134              25      4.188925   \n",
       "14         197                100              13      4.485876   \n",
       "15         605                260              35      4.799625   \n",
       "16         385                149              18      4.092219   \n",
       "17         420                177              15      4.112000   \n",
       "18          72                 53               7      4.409091   \n",
       "19         181                 97              11      4.300000   \n",
       "\n",
       "    grammar_errors  lsa_score  \n",
       "0               16   0.645607  \n",
       "1               25   0.746648  \n",
       "2               17   0.676962  \n",
       "3               29   0.729032  \n",
       "4               17   0.728624  \n",
       "5               17   0.722233  \n",
       "6                6   0.664469  \n",
       "7                7   0.930333  \n",
       "8               13   0.721024  \n",
       "9               20   0.624251  \n",
       "10               5   0.799711  \n",
       "11              49   0.783211  \n",
       "12              27   0.578172  \n",
       "13              51   0.687472  \n",
       "14              11   0.702640  \n",
       "15              21   0.866593  \n",
       "16              12   0.693132  \n",
       "17              16   0.852123  \n",
       "18              34   0.628601  \n",
       "19              12   0.752274  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lsa['lsa_score'] = df_lsa['essay'].apply(lsa_score)\n",
    "\n",
    "df_lsa.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF Matrix Shape: (1763, 10000)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer for the training data\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Vectorize document using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                   stop_words='english',\n",
    "                                   ngram_range = (1,3),\n",
    "                                   tokenizer = tokenizer.tokenize,\n",
    "                                   max_features=10000,\n",
    "                                   max_df=0.8,\n",
    "                                   min_df=5)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_lsa[\"essay\"])\n",
    "print(f\"Train TFIDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# TFIDF to SVD\n",
    "svd_model = TruncatedSVD(n_components=100,\n",
    "                         n_iter=200,\n",
    "                         random_state=69)\n",
    "    \n",
    "svd = svd_model.fit_transform(tfidf_matrix)\n",
    "#normalized_svd = Normalizer(copy=False).fit_transform(svd)\n",
    "print(type(svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_features = df_lsa[['word_count', 'unique_word_count', 'sentence_count', 'avg_word_len', 'grammar_errors', 'lsa_score']]\n",
    "\n",
    "x_features = np.concatenate((x_df_features.to_numpy(), svd), axis=1)\n",
    "y_features = df_lsa['actual_score'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_features, y_features, test_size = 0.2, train_size = 0.8, random_state = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Linear Regression Model\n",
      "Building SVR Model\n",
      "Building Decision Tree Model\n",
      "Building Bayesian Regressor\n",
      "Building AdaBoost Regressor\n",
      "Building Random Forest Regressor\n",
      "Building Gradient Boosting Regressor\n",
      "Building Logistic Regression Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear regression, LogisticRegression, SVR\n",
    "\n",
    "print(\"Building Linear Regression Model\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building SVR Model\")\n",
    "svr_model = SVR()\n",
    "svr_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Decision Tree Model\")\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Bayesian Regressor\")\n",
    "bayes_model = BayesianRidge()\n",
    "bayes_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building AdaBoost Regressor\")\n",
    "ada_model = AdaBoostRegressor(n_estimators=100)\n",
    "ada_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Random Forest Regressor\")\n",
    "ran_model = RandomForestRegressor()\n",
    "ran_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Gradient Boosting Regressor\")\n",
    "grad_model = GradientBoostingRegressor(n_estimators=200)\n",
    "grad_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Building Logistic Regression Model\")\n",
    "log_model = LogisticRegression(solver=\"saga\", max_iter=10000)\n",
    "log_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [lr_model.predict(x_test),\n",
    "               svr_model.predict(x_test),\n",
    "               tree_model.predict(x_test),\n",
    "               bayes_model.predict(x_test),\n",
    "               ada_model.predict(x_test),\n",
    "               ran_model.predict(x_test),\n",
    "               grad_model.predict(x_test),\n",
    "               log_model.predict(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Mean Absolute Error: 0.6526003711290022\n",
      "Mean Squared Error: 0.7103185147316082\n",
      "Root Mean Squared Error: 0.8428039598457094\n",
      "R2 score: 0.7107316027459067\n",
      "\n",
      "Model 1\n",
      "Mean Absolute Error: 0.6413297427444046\n",
      "Mean Squared Error: 0.6365106386053946\n",
      "Root Mean Squared Error: 0.7978161684281627\n",
      "R2 score: 0.7407889440497943\n",
      "\n",
      "Model 2\n",
      "Mean Absolute Error: 0.6912181303116147\n",
      "Mean Squared Error: 1.048158640226629\n",
      "Root Mean Squared Error: 1.0237961907658326\n",
      "R2 score: 0.5731504055741112\n",
      "\n",
      "Model 3\n",
      "Mean Absolute Error: 0.6494706226345482\n",
      "Mean Squared Error: 0.6963751743662591\n",
      "Root Mean Squared Error: 0.8344909672166974\n",
      "R2 score: 0.7164098550175328\n",
      "\n",
      "Model 4\n",
      "Mean Absolute Error: 0.6799157946086015\n",
      "Mean Squared Error: 0.6780406364736538\n",
      "Root Mean Squared Error: 0.8234322294358254\n",
      "R2 score: 0.7238763679699545\n",
      "\n",
      "Model 5\n",
      "Mean Absolute Error: 0.6102266288951842\n",
      "Mean Squared Error: 0.5878518413597734\n",
      "Root Mean Squared Error: 0.7667149674812495\n",
      "R2 score: 0.7606046351793873\n",
      "\n",
      "Model 6\n",
      "Mean Absolute Error: 0.6129937615907657\n",
      "Mean Squared Error: 0.6029878999913351\n",
      "Root Mean Squared Error: 0.7765229552249792\n",
      "R2 score: 0.7544406632002109\n",
      "\n",
      "Model 7\n",
      "Mean Absolute Error: 0.7903682719546742\n",
      "Mean Squared Error: 1.385269121813031\n",
      "Root Mean Squared Error: 1.1769745629422206\n",
      "R2 score: 0.4358663468263253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, pred in enumerate(predictions):\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r_score = r2_score(y_test, pred)\n",
    "\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"R2 score: {r_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chaldea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = [cross_val_score(lr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(svr_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(tree_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(bayes_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ada_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(ran_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(grad_model, x_train, y_train, cv=10).mean(),\n",
    "          cross_val_score(log_model, x_train, y_train, cv=10).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0\n",
      "Overall Score: 0.6762906202765445\n",
      "\n",
      "Model 1\n",
      "Overall Score: 0.7204772810151014\n",
      "\n",
      "Model 2\n",
      "Overall Score: 0.4326863192518579\n",
      "\n",
      "Model 3\n",
      "Overall Score: 0.7001464221689246\n",
      "\n",
      "Model 4\n",
      "Overall Score: 0.702954700103647\n",
      "\n",
      "Model 5\n",
      "Overall Score: 0.7219493410579478\n",
      "\n",
      "Model 6\n",
      "Overall Score: 0.7071275213281665\n",
      "\n",
      "Model 7\n",
      "Overall Score: 0.48723404255319147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, score in enumerate(scores):\n",
    "    print(f\"Model {idx}\")\n",
    "    print(f\"Overall Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "### Best Performing Model for Essay Set 1\n",
    "**Model 5 (Random Forest Regressor, no LSA score)**<br>\n",
    "Mean Absolute Error: 0.6238935574229693<br>\n",
    "Mean Squared Error: 0.6290591036414567<br>\n",
    "Root Mean Squared Error: 0.7931324628594247<br>\n",
    "R2 score: 0.7533473815852623<br><br>\n",
    "\n",
    "**Model 5 (Random Forest Regressor, no LSA score, 10-Fold Validation)**<br>\n",
    "Overall Score: 0.7219474896299766<br><br>\n",
    "\n",
    "**Model 5 (Random Forest Regressor, with LSA score)**<br>\n",
    "Mean Absolute Error: 0.563399433427762<br>\n",
    "Mean Squared Error: 0.5285376770538244<br>\n",
    "Root Mean Squared Error: 0.727005967687903<br>\n",
    "R2 score: 0.7660819899557456<br><br>\n",
    "\n",
    "**Model 5 (Random Forest Regressor, with LSA score, 10-Fold Validation)**<br>\n",
    "Overall Score: 0.7126748931617651<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d888b7e27d9d4f8fccada6ae7e4260d620cc5227dee391d6d7538000bf76028"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
